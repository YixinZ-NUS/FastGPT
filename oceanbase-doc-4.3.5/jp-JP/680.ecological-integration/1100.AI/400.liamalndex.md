|description|  |
|---|---|
|keywords| |
|dir-name|LlamaIndex|
|dir-name-en|LlamaIndex|
|tenant-type|MySQL Mode|

# OceanBaseデータベースのベクトル検索機能とLlamaIndexの連携

OceanBaseデータベースは、バージョンV4.3.3からベクトル型データの格納、ベクトルインデックス、そして埋め込み(embedding)ベクトル検索機能をサポートしています。これにより、ベクトル化したデータをOceanBaseに保存し、その後の検索処理で直接利用できるようになりました。

LlamaIndexは、LLM(プロキシとワークフローを含む)を使用して、コンテキスト強化型の生成AIアプリケーションを構築するためのフレームワークです。データコネクタ、データインデックス、プロキシ、可観測性/評価統合、ワークフローなどの機能を提供します。

本記事は、Qwen APIを活用し、OceanBaseデータベースのベクトル検索機能とQwen、LlamaIndexを連携させて、ドキュメント質問応答を実現する方法を説明します。

## 前提条件

* OceanBaseデータベースのデプロイが完了し、MySQLモードのユーザーテナントが作成されていること。テナント作成の詳細については、[テナントの作成](https://en.oceanbase.com/docs/common-oceanbase-database-10000000001971591)を参照してください。

  * ご利用の環境には、既に利用可能なMySQLテナント、MySQLデータベース、およびアカウントが存在し、データベースアカウントには読み書き権限が付与されていること。
  * ベクトル検索機能を有効にするには、テナントで`ob_vector_memory_limit_percentage`構成パラメータが設定されていることを確認してください。V4.3.5 BP3より前のバージョンでは、この値を`30`に設定することを推奨します。V4.3.5 BP3以降のバージョンでは、デフォルト値である`0`のままにしておくことを推奨します。より正確にこのパラメータを設定する必要がある場合は、[ob_vector_memory_limit_percentage](https://en.oceanbase.com/docs/common-oceanbase-database-10000000001976671)を参照してこの値を計算してください。

* Python 3.9以降のバージョンをインストールしていること。
* 必要な依存関係をインストールする:

    ```shell
    python3 -m pip install llama-index-vector-stores-oceanbase llama-index
    python3 -m pip install llama-index-embeddings-dashscope
    python3 -m pip install llama-index-llms-dashscope
    ```

* Qwen APIキーを準備します。

## ステップ1：データベース接続情報を取得する

OceanBaseデータベースのデプロイ担当者または管理者に連絡し、対応するデータベース接続文字列を取得します。例：

```sql
obclient -h$host -P$port -u$user_name -p$password -D$database_name
```

**パラメータの説明：**

* `$host`：OceanBaseデータベースへの接続IPアドレスです。OceanBaseデータベースプロキシ(OceanBase Database Proxy、ODP)経由で接続する場合はODPのアドレスを、直接接続する場合はOBServerノードのIPアドレスを使用します。
* `$port`：OceanBaseデータベースの接続ポートです。ODP経由の接続ではデフォルトで`2883`が使用されますが、ODPのデプロイ時にカスタマイズ可能です。直接接続の場合はデフォルトで`2881`が使用され、OceanBaseデータベースのデプロイ時にカスタマイズできます。
* `$database_name`：アクセスするデータベースの名前です。

    <main id="notice" type='notice'>
        <h4>注意</h4>
        <p>テナントに接続するユーザーは、データベースに対する <code>CREATE</code>、<code>INSERT</code>、<code>DROP</code>、および <code>SELECT</code> 権限が付与されていなければなりません。ユーザー権限の詳細については、<a href="https://en.oceanbase.com/docs/common-oceanbase-database-10000000001974758">MySQLモードの権限分類</a>を参照してください。</p>
    </main>

* `$user_name`：テナントへの接続アカウントです。ODP経由で接続する場合の一般的な形式は`ユーザー名@テナント名#クラスタ名`または`クラスタ名:テナント名:ユーザー名`。直接接続の場合は`ユーザー名@テナント名`となります。
* `$password`：アカウントのパスワードです。

接続文字列の詳細については、[OBClientを使用してOceanBaseテナントに接続する](https://en.oceanbase.com/docs/common-oceanbase-database-10000000001971649)を参照してください。

## ステップ2：AIアシスタントを作成する

### Qwen APIキーの環境変数を設定をする

[Qwen APIキー](https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key)を取得し、[APIキーを環境変数に設定する](https://help.aliyun.com/zh/model-studio/developer-reference/configure-api-key-through-environment-variables)します。

```shell
export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_API_KEY"
```

### サンプルデータをダウンロードする

```shell
mkdir -p '/root/llamaindex/paul_graham/'
wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O '/root/llamaindex/paul_graham/paul_graham_essay.txt'
```

### データテキストを読み込む

```python
import os
from pyobvector import ObVecClient
from llama_index.core import Settings
from llama_index.embeddings.dashscope import DashScopeEmbedding
from llama_index.core import (
SimpleDirectoryReader,
load_index_from_storage,
VectorStoreIndex,
StorageContext,
)
from llama_index.vector_stores.oceanbase import OceanBaseVectorStore
from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels
#set ob client
client = ObVecClient(uri="127.0.0.1:2881", user="root@test",password="",db_name="test")
# Global Settings
Settings.embed_model = DashScopeEmbedding()
# config llm model
dashscope_llm = DashScope(
    model_name=DashScopeGenerationModels.QWEN_MAX,
    api_key=os.environ.get("DASHSCOPE_API_KEY", ""),
)
# load documents
documents = SimpleDirectoryReader("/root/llamaindex/paul_graham/").load_data()
oceanbase = OceanBaseVectorStore(
    client=client,
    dim=1536,
    drop_old=True,
    normalize=True,
)

storage_context = StorageContext.from_defaults(vector_store=oceanbase)
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context
)
```

## ベクトル検索

この手順では、ドキュメント`paul_graham_essay.txt`に対して、クエリ`“What did the author do growing up?”`を実行する方法を示します。

```shell
# set Logging to DEBUG for more detailed outputs
query_engine = index.as_query_engine(llm=dashscope_llm)
res = query_engine.query("What did the author do growing up?")
res.response
```

期待される結果：

```python
'Growing up, the author worked on writing and programming outside of school. In terms of writing, he wrote short stories, which he now considers to be awful, as they had very little plot and focused mainly on characters with strong feelings. For programming, he started in 9th grade by trying to write programs on an IBM 1401 at his school, using an early version of Fortran. Later, after getting a TRS-80 microcomputer, he began to write more practical programs, including simple games, a program to predict the flight height of model rockets, and a word processor that his father used for writing.'
```
