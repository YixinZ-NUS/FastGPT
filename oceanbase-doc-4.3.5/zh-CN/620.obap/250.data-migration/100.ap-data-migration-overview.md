# 数据迁移概述

数据迁移是数据管理的核心任务，涉及将数据从一个系统或存储环境转移到另一个系统，以满足业务扩展、系统升级、数据整合或合规需求。根据数据源与目标系统的类型、数据状态及迁移场景，数据迁移可分为三大类型：**异构数据离线迁移**、**同构数据库迁移**和**大数据生态集成迁移**。

## 异构数据迁移

异构数据迁移指将数据从外部数据源（如 MySQL、Oracle、PostgreSQL 等关系型数据库，或各类 NoSQL 数据库）迁移至 OceanBase 数据库。这类迁移需要解决数据格式转换、结构适配和跨平台兼容性等问题。在这类数据迁移场景中，需要关注：

+ **Schema 与数据类型映射**：需要精确定义源端和目标端（OceanBase）之间的数据结构和类型映射规则，例如，将 MySQL 的 `DATETIME` 转换为 OceanBase 的 `TIMESTAMP`。
+ **性能优化**：针对海量数据的迁移，采用高效的迁移策略，如并行处理、分片迁移和批量加载，以缩短迁移窗口。
+ **数据一致性校验**：在迁移完成后，需通过哈希校验、抽样对比或全量对比等方法，确保数据的完整性和准确性。

更多有关异构数据迁移的信息，请参见[异构数据迁移](200.heterogeneous-data-migration.md)。

## 同构数据迁移

同构数据迁移主要指在 OceanBase 数据库集群之间进行数据迁移。这类迁移通常发生在以下场景：

+ **版本升级**：将数据从旧版本的 OceanBase 集群迁移至新版本，以获取新功能或提升性能。
+ **TP 数据库与 AP 数据库间迁移**：将数据从 OceanBase 事务型（TP）数据库迁移至分析型（AP）数据库，以支持实时分析或 BI 应用等。

由于源端和目标端均为 OceanBase，此类迁移通常无需进行数据格式转换，但仍需关注：

+ **迁移期间的数据一致性**：通过事务日志或快照机制保证迁移过程中数据的一致性。
+ **停机时间控制**：根据业务对 RTO/RPO 的要求，选择合适的迁移方案以最小化或避免业务中断。
+ **版本兼容性**：在跨版本迁移时，需注意不同版本间 SQL 语法、系统参数或内部实现的细微差异。

更多有关同构数据迁移的信息，请参见[同构数据迁移](300.homogeneous-data-migration.md)。

## 大数据生态集成

大数据生态集成迁移特指数据源已存在于离线数仓（如 Hive、Spark、HBase、Parquet/ORC 文件等）或大数据平台组件中，而非直接来源于传统 OLTP 数据库或实时消息队列。这类迁移的核心目标是将这些已在大数据生态中处理或存储的数据，高效、可靠地集成或迁移到 OceanBase AP 数据库或其他目标系统，以支持更高级的分析、实时查询或与其他业务系统的协同。这类迁移通常发生在以下场景：

+ **离线数仓数据需要同步到实时分析系统**：将 Hive、Spark 等离线数仓中的数据同步到实时数据处理平台（如 Flink、Kafka），以支持实时数据处理和流式分析。
+ **离线数据需归档到长期存储**：将离线数仓中的历史数据归档到长期存储（如云对象存储 S3、阿里云 OSS），以降低存储成本并满足数据保留策略。
+ **离线数据需与外部系统集成**：将 Hive 数据迁移到 Snowflake 等外部系统进行跨团队协作，或将大数据平台的数据与业务系统进行集成。

在这类数据迁移场景中，需要关注：

+ **存储格式转换**：如 Parquet 到 Avro 的格式转换，需要考虑不同格式的压缩率、查询性能和兼容性。
+ **海量数据的高效传输**：使用 DistCp、Spark 等分布式工具，通过并行处理和网络优化来提升传输效率。
+ **元数据同步**：确保源端（如 Hive Metastore）的元数据（表结构、分区信息等）能与 OceanBase AP 中的目标表结构正确同步和映射。

更多有关大数据生态集成的信息，请参见[大数据生态集成](400.big-data-integration.md)。
