|description||
|---|---|
|keywords||
|dir-name||
|dir-name-en||
|tenant-type||

# ASH 最佳实践

本文通过三个零基础实验与三大分析维度，结合真实故障案例，展示利用 [G]V$OB_ACTIVE_SESSION_HISTORY 视图快速定位 CPU 突增、I/O 瓶颈、分布式执行阻塞等性能问题。

## ASH 实现原理

ASH 就像数据库的自动录像机，每秒都会给所有正在工作的会话（比如执行 SQL 的会话）拍摄状态快照，这些快照都存储在 GV$OB_ACTIVE_SESSION_HISTORY 这个系统视图中。具体的实现原理如下：

1. **活跃会话过滤机制**

   * 记录所有数据库中执行的任务，并赋予其唯一标识 session_id，包括：

     * 用户客户端连接数据库执行 SQL 请求。
     * 内部 RPC 执行。
     * 后台线程执行任务，如转储线程、Clog 线程、定时器线程等。

   * 只记录活跃会话的状态，空闲会话不会被记录，包括：

     * 正在执行 SQL 的会话认为是活跃的。如果某会话处于 Sleep 状态，没有处理 SQL 请求，则视为空闲状态，不会记录。
     * 后台线程如果没有执行任务、或在等待新任务调度，则视为空闲状态，不会记录。
     * 正在等待资源（如锁、磁盘 I/O）的会话，ASH 会标记其等待事件（如 db file data read）。

2. **周期性采样机制**

   每个 OBServer 内部，有一个专门的 ASH 线程，以 1 秒为周期，访问数据库内全部活跃会话，并记录其状态，其中：

   * `GV$OB_ACTIVE_SESSION_HISTORY` 中每行的数据代表一个活跃会话在某时刻的状态。
   * 如果某个会话的工作时间非常短（比如不到 1 秒），就像拍照时眨眼的人，可能无法被 ASH 捕捉到。对这种场景，建议重复负载并扩大查询时间范围，这样 ASH 的统计结果才更可靠。

3. **环形缓冲区设计**

    ASH 快照数据保存在 30MB 的循环缓冲区。存储数据超过 30MB 后，会自动覆盖最旧的数据。在 V4.2.5 BP3 版本中实现了覆盖前 ASH 数据自动归档为 WR 的功能。但在之前版本，仍有可能出现 ASH 历史数据丢失，当我们希望保存最新的 ASH 记录时，可以手动触发 WR 快照：

    ```shell
    CALL DBMS_WORKLOAD_REPOSITORY.CREATE_SNAPSHOT();
    ```

    执行该命令后，当前时刻还未持久化到 WR 中的 ASH 快照数据以 10:1 的比例持久化。

## ASH 零基础实验

准备了三个零基础实验，打开数据库的 ASH 时间线回溯，让你快速感受 ASH 的魅力。

**实验 1：查看最近 10 秒数据库状态（实时监控）。**

查看数据库最近 10 秒在忙什么：

```shell
-- 查看系统中最近 10 秒钟的运行状况
SELECT
  sample_time AS 时间, -- 精确到微秒的时间戳
  session_id AS 会话ID, -- 唯一标识会话的 ID
  CASE WHEN session_state = 'ON CPU' THEN '工作中' ELSE '等待中' END AS 状态, -- 工作状态：CPU 忙碌或等待资源
  event AS 等待原因 -- 具体的等待事件（如锁、I/O 等）
FROM v$ob_active_session_history
WHERE sample_time > now() - 10 -- 最近 10 秒钟
      AND session_type="FOREGROUND"
ORDER BY sample_time DESC;
```

得到结果如下：

```shell
+----------------------------+------------+-----------+------------------------+
| 时间                       | 会话ID     | 状态      | 等待原因                 |
+----------------------------+------------+-----------+------------------------+
| 2025-06-11 20:16:15.307564 | 3221931170 | 等待中    | px loop condition wait  |
| 2025-06-11 20:16:14.285204 | 3221928286 | 工作中    |                         |
| 2025-06-11 20:16:14.285204 | 3221923503 | 等待中    | wait in request queue   |
| 2025-06-11 20:16:14.285204 | 3221923627 | 等待中    | db file data read       |
| 2025-06-11 20:16:14.285204 | 3221927472 | 等待中    | sync rpc                |
| 2025-06-11 20:16:13.262695 | 3221929034 | 工作中    |                         |
| 2025-06-11 20:16:12.240768 | 3221927472 | 工作中    |                         |
+----------------------------+------------+-----------+------------------------+
```

发现：

* 会话 3221931170 在等待 px 执行完成（px loop condition wait）
* 会话 3221923627 在等待读 IO 完成（db file data read）
* 会话 3221927472 在 20:16:12 时还在工作中，而到 2 秒后，它在等待 RPC 返回结果（sync rpc）

**实验 2：统计最近 10 分钟最活跃会话（负载分布）。**

查看在最近 10 分钟忙碌的会话：

```shell
-- 统计最近 10 分钟最活跃的会话
SELECT
  session_id AS 会话ID,
  COUNT(*) AS 工作秒数
FROM v$ob_active_session_history
WHERE sample_time > now() - 600    -- 最近10分钟
      AND session_type='FOREGROUND'
GROUP BY session_id
ORDER BY 工作秒数 DESC limit 3;
```

得到结果如下：

```shell
+------------+--------------+
| 会话ID     | 工作秒数      |
+------------+--------------+
| 3221977564 |          283 |
| 3221972645 |          142 |
| 3221916432 |          77  |
+------------+--------------+
```

发现会话 3221977564 在过去 10 分钟里活跃了 283 秒。如果过去时段只有这三个 Session，那么会话 3221977564 在总数据库负载中的贡献比例为 `283 / (283 + 142 +77)= 56%`

**实验 3：定位历史时段高负载 SQL（性能分析）。**

查看过去一段时间最繁忙的 SQL：

```shell
-- 查询过去时间段中，执行负载最高的 sql_id
SELECT
  SQL_ID,
  COUNT(*) AS 工作秒数
FROM v$ob_active_session_history
WHERE sample_time BETWEEN
      '2025-06-11 10:32:08'
      AND '2025-06-11 11:32:07' -- 可修改时间为实际想观察的时段
GROUP BY sql_id
ORDER BY 工作秒数 DESC limit 3;
```

得到结果如下：

```shell
+----------------------------------+--------------+
| SQL_ID                           | 工作秒数     |
+----------------------------------+--------------+
| 1D0BA376E273B9D622641124D8C59264 |        91265 |
| 19AAD9F2FE3CE0023298AB83F7E75775 |        13608 |
| 7BE7497CCCFE8978AD6B92A938D43929 |        13098 |
+----------------------------------+--------------+
```

发现过去一段时间最繁忙的 SQL 为 1D0BA376E273B9D622641124D8C59264，执行了 91265 秒。

## ASH 分析法

`V$OB_ACTIVE_SESSION_HISTORY` 视图提供秒级全量会话快照，无需预先配置即可回溯最近 7 天的运行状态。以下三个维度是本文重点分析的核心指标，它们共同描绘了数据库的运行状况：

* AAS（Average Active Session）：即数据库每秒平均会话数，反映了数据库负载状况，AAS 越高，负载越大。
* 等待事件（Wait Event）：当活跃会话在等待某种系统资源时，比如锁、网络 RPC、I/O 等，会进入等待事件状态。与此相反，不处于等待事件状态的会话则处于工作状态（ON_CPU）。
* 执行阶段：SQL 在数据库内执行会经历各种阶段，如 IN_PARSE、IN_SQL_OPTIMIZE、IN_SQL_EXECUTION、IN_STORAGE_READ 等，SQL 在执行过程中也会调动各种资源，同样会进入各种状态，如IN_RPC_ENCODE、IN_CONNECTION_MRG、IN_SEQUENCE_LOAD 等。

通过 ASH 分析法，可以还原历史时段数据库的整体运行状况，而不仅仅是监控指标代表的某数据库微观维度。这就是 ASH 和传统监控的最大区别。

### AAS 分析

根据 ASH 只会记录数据库系统中活跃会话的特性，可以统计出数据库每秒钟活跃会话的数量，活跃会话越多，代表着系统越繁忙、负载越高。使用指标 AAS 来表示每秒平均会话数，从而掌握数据库精确到秒级的负载状况。

AAS 可以这样查询：

```shell
SELECT
    DATE_FORMAT(SAMPLE_TIME, '%Y-%m-%d %H:%i') AS TIME_SLOT,
    -- 以每分钟为区间划分
    COUNT(*) / 60 as AAS
    -- 每分钟平均活跃会话数
FROM V$OB_ACTIVE_SESSION_HISTORY
WHERE sample_time BETWEEN
      '2025-03-13 10:32:08'
      AND '2025-03-13 10:35:07' -- 可修改时间为实际想观察的时段
GROUP BY TIME_SLOT ORDER BY TIME_SLOT;
```

得到结果如下：

```shell
+------------------+------+
| TIME_SLOT        | AAS  |
+------------------+------+
| 2025-03-13 10:32 |  2   |
| 2025-03-13 10:33 |  13  |
| 2025-03-13 10:34 |  15  |
| 2025-03-13 10:35 |  1   |
+------------------+------+
```

在查询时段里，10:33 至 10:34 活跃会话数有突增，有可能是用户 SQL 突发流量，也可能是突然被调度的后台任务消耗了过多系统资源，具体是否出现资源瓶颈，需要结合租户规格、系统状况等因素综合考虑。需要注意的是，`V$OB_ACTIVE_SESSION_HISTORY` 视图只保存了系统最新一段时间的历史快照数据，因此，在查询前，有必要确认视图中的历史数据是否已经被覆盖了。

下面，通过一个实际案例来阐述 AAS 如何应用在问题诊断排查中。

#### 案例 1：定位后台任务导致的 CPU 突增

某互联网公司使用 OceanBase 数据库，租户配置为 3c12g，数据库在早上 8:00 开始出现 CPU 异常升高，用户收到 OCP 告警之后，在 8:14 分左右，将数据库租户配置升级到 6c20g，租户 cpu 依旧被打满。问题持续 2 小时后消失，故障期间 OCP 显示用户 QPS 并没有明显变化。

可以通过查询 ASH 视图来定位负载突变的原因。由于已经在 OCP 上明确了故障时段，因此我们可以直接对故障时段的 AAS 进行分析。

```shell
SELECT
    SVR_IP,
    SVR_PORT,
    SESSION_TYPE,
    COUNT(*) as AAS
    -- 活跃会话总时间，除以时长得到 AAS
FROM V$OB_ACTIVE_SESSION_HISTORY
WHERE sample_time BETWEEN
      '2025-03-13 8:00:00'
      AND '2025-03-13 10:00:00'
GROUP BY SVR_IP, SVR_PORT, SESSION_TYPE ORDER BY AAS DESC limit 5;
```

查询结果如下：

```shell
+--------------+----------+--------------+-------+
| SVR_IP       | SVR_PORT | SESSION_TYPE | AAS   |
+--------------+----------+--------------+-------+
|  xx.xx.xx.x6 |    46775 | BACKGROUND   | 59633 |
|  xx.xx.xx.x6 |    46775 | FOREGROUND   |  1324 |
|  xx.xx.xx.x5 |    46779 | FOREGROUND   |   512 |
|  xx.xx.xx.x9 |    46771 | FOREGROUND   |   412 |
|  xx.xx.xx.x9 |    46771 | BACKGROUND   |   403 |
|  xx.xx.xx.x5 |    46779 | BACKGROUND   |   379 |
+--------------+----------+--------------+-------+
```

需要理解的是，由于 OceanBase 数据库是分布式数据库，一般情况下，它都由多个节点组成，因此在查询 ASH 时，最好加上 SVR_IP 和 SVR_PORT，这两个字段可以唯一标识一个 OBServer 节点。
SESSION_TYPE 标识会话的类型，只有 FOREGROUND 前台和 BACKGROUND 后台两种类型。FOREGROUND 前台会话是用户与 OceanBase 数据库创建链接产生的会话，它用来执行用户发送的 SQL 请求。BACKGROUND 后台会话会执行 OceanBase 数据库内部任务，包括转储、RPC、PX 任务等。
从查询结果中我们不难发现，负载主要集中在 xx.xx.xx.x6:46775 节点，其 AAS=59633/(3600 * 2) = 8.28，远超租户规格 3CPU。扩容后 6CPU 也难以满足后台会话的需求。
下一步，需要排查在 xx.xx.xx.x6:46775 节点上，到底是什么后台任务占用了租户这么多 CPU。在 ASH 里，能标识后台会话属性的有很多字段，常用的有 PROGRAM、GROUP_ID 等。PROGRAM 标识会话的属性，GROUP_ID 标识会话属于哪个资源组。因此我们构造如下 SQL ：

```shell
SELECT
    PROGRAM,
    GROUP_ID,
    COUNT(*) as CNT
FROM V$OB_ACTIVE_SESSION_HISTORY
WHERE SESSION_TYPE = 'BACKGROUND'
      AND SVR_IP = 'xx.xx.xx.x6'
      AND SVR_PORT = '46775'
      AND sample_time BETWEEN
      '2025-03-13 8:00:00'
      AND '2025-03-13 10:00:00'
GROUP BY PROGRAM, SESSION_TYPE ORDER BY CNT DESC limit 10;
```

查询结果如下：

```shell
+-------------------------+----------+-------+
| PROGRAM                 | GROUP_ID | CNT   |
+-------------------------+----------+-------+
| T1002_RPC_REQUEST       |       28 | 53278 |
| T1002_LogService        |        0 |  2793 |
| T1002_DAG               |        0 |  1425 |
| T1002_IOManager         |        0 |  1268 |
| T1002_LSRecoveryService |        0 |   438 |
+-------------------------+----------+-------+
```

查询后发现，xx.xx.xx.x6:46775 节点上，主要的后台负载压力来自于 RPC 执行，并且其资源组是 28。28 资源组是 OBServer 内部任务专属资源组，它分配给了 DBMS_SCHEDULER 模块。由此，我们还原了故障时段的数据库现场：由于某种原因 DBMS_SCHEDULER 模块发送了大量 RPC 请求到 xx.xx.xx.x6:46775 节点，导致 OBServer 节点 CPU 陡增。后续的排查证实，DBMS_SCHEDULER 在早 8 点运行了 SQL 统计信息收集任务，由于配置失误，导致所有的统计信息收集任务被调度在同一节点，导致 CPU 陡增。
传统手段排查这问题需要耗费的时间较长，由于监控没有更多的信息提供，因此需要从 OBServer 运行日志入手，根据日志异常进行分析，逐渐推导到 DBMS_SCHEDULER 模块。反观 ASH，虽然没有专门针对 DBMS_SCHEDULER 模块的监控配置，但通过 AAS 维度结合后台属性，顺利定位到故障原因。

### 等待事件分析

数据库的任何活跃会话在执行过程中，要么处于等待状态，即等待某个资源变得可用或某个条件得到满足才能继续执行，要么处于 ON CPU 工作状态（例如解析 SQL 语句、处理 SQL 算子、等待内存访问完成等）。在 OceanBase 数据库中，我们使用等待事件（wait event）来标记一个活跃会话处于等待状态。显然，定位数据库回话执行过程中发生过哪些等待事件、时间占比是多少。对数据库调优至关重要。

我们可以使用 SQL 查询某段时间前台会话的等待事件占比情况：

```shell
SELECT
    SESSION_STATE,
    EVENT,
    COUNT(*) as CNT
FROM V$OB_ACTIVE_SESSION_HISTORY
WHERE SESSION_TYPE = 'FOREGROUND'
      AND SVR_IP = 'xx.xx.xx.x7'
      AND SVR_PORT = '2828'
      AND sample_time BETWEEN
      '2025-06-14 8:15:00'
      AND '2025-06-14 10:31:00'
GROUP BY SESSION_STATE, EVENT ORDER BY CNT DESC limit 10;
```

查询结果如下：

```shell
+---------------+--------------------------------------------------------+--------+
| SESSION_STATE | EVENT                                                  | CNT    |
+---------------+--------------------------------------------------------+--------+
| WAITING       | wait in request queue                                  | 227184 |
| ON CPU        |                                                        |  76111 |
| WAITING       | tx commiting wait                                      |  54866 |
| WAITING       | exec inner sql wait                                    |  10292 |
| WAITING       | sync rpc                                               |   6602 |
| WAITING       | px loop condition wait                                 |   3234 |
| WAITING       | db file data read                                      |    395 |
| WAITING       | row lock wait                                          |    158 |
| WAITING       | default condition wait                                 |    143 |
+---------------+--------------------------------------------------------+--------+
```

可见，在查询时段，租户队列积压十分严重（wait in request queue），会极大影响 SQL 执行时间。

#### 案例 2：定位磁盘 I/O 瓶颈

某教育公司在某天下午出现数据库请求延迟变高，引起客诉，DBA 通过 OCP 监控发现故障时段数据盘读带宽打满，推测延迟变高的原因是 SQL 读磁盘过多导致的。但观察故障时段的 TOP SQL ，并没有发现和正常时段有什么区别，因此无法判断读磁盘流量是哪条 SQL 导致的。

可以通过查询 ASH 等待事件来定位读磁盘流量和哪些 SQL 有关：

```shell
OceanBase(root@oceanbase)>select svr_ip, svr_port, con_id as tenant_id, sql_id, event, count(*) as cnt from GV$OB_ACTIVE_SESSION_HISTORY ash where wait_class in ('SYSTEM_IO', 'USER_IO') and sample_time between "2025-01-29 10:35:45" and "2024-01-29 10:45:47" group by svr_ip, svr_port, tenant_id, sql_id, event order by cnt desc limit 100;
```

查询结果如下：

```shell
+----------------+----------+-----------+------------------------------------+-----------------------+-------+
| svr_ip         | svr_port | tenant_id | sql_id                             | event                 | cnt   |
+----------------+----------+-----------+------------------------------------+-----------------------+-------+
|   xx.xx.xx.xx  |     2882 |      1002 | 18AB15790D884E6A4823C892456E8650   | db file data read     | 24511 |
|   xx.xx.xx.xx  |     2882 |      1002 | DBCB5D52A5E14718345165081BC7EAE4   | db file data read     | 17859 |
|   xx.xx.xx.xx  |     2882 |      1002 | 32AB97A0126F566064F84DDDF4936F82   | db file data read     | 15463 |
|   xx.xx.xx.xx  |     2882 |      1002 | 752F9D90CE614E180E965438E4B5A87D   | db file data read     | 11674 |
|   xx.xx.xx.xx  |     2882 |      1002 | NULL                               | palf write            | 1581  |
|   xx.xx.xx.xx  |     2882 |      1002 | NULL                               | db file compact read  | 193   |
+----------------+----------+-----------+------------------------------------+-----------------------+-------+
```

可见，有 4 个 SQL 在执行过程中有大量 IO 相关等待事件，会极大的拖慢 SQL 响应时间。

### 执行阶段分析

SQL 的执行流程分为很多阶段，在 OceanBase 数据库 V4.2.5 BP3 版本上，共划分了 17 种执行阶段，在 v$ob_active_session_history 上用 17 列来表示这些执行阶段，当 SQL 在采样时刻处于该阶段，列值为'Y'，不处于该阶段，列值为'N'。

1. IN_PARSE：系统正在做 SQL 语句的语法解析。
2. IN_PL_PARSE：系统正在解析 PL/SQL 代码（如存储过程、函数）。
3. IN_PLAN_CACHE：系统正在检索或生成 SQL 的执行计划。
4. IN_SQL_OPTIMIZE：系统正在优化 SQL 执行路径。
5. IN_SQL_EXECUTION：SQL 正在实际执行（如访问数据、计算结果）。
6. IN_PX_EXECUTION：系统正在通过多线程/节点并行执行 SQL（并行查询）。
7. IN_SEQUENCE_LOAD：系统正在为自增列（如 ID 字段）或序列 SEQUENCE 生成唯一值。
8. IN_COMMITTING：系统正在提交事务。
9. IN_STORAGE_READ：系统正在从存储引擎读取数据（如表、索引）。
10. IN_STORAGE_WRITE：系统正在向存储引擎写入数据。
11. IN_REMOTE_DAS_EXECUTION：系统正在使用 DAS 执行引擎做分布式执行。
12. IN_FILTER_ROWS：系统的存储引擎正在执行下压算子的执行（如过滤、聚合）。
13. IN_RPC_ENCODE：系统正在将请求/响应数据进行编码
14. IN_RPC_DECODE：系统正在将请求/响应数据进行解码
15. IN_CONNECTION_MGR：系统正在处理会话的创建或关闭（连接池管理）
16. IN_PLSQL_COMPILATION：系统正在编译或 PL/SQL 代码（如存储过程）
17. IN_PLSQL_EXECUTION：系统正在执行 PL/SQL 代码（如存储过程）

在 OceanBase 数据库中，等待事件间是存在相互包含关系的，例如处于 IN_SQL_EXECUTION SQL 执行阶段的会话，可能正在访问存储层，即同时处于 IN_STORAGE_READ/IN_STORAGE_WRITE 阶段。

#### 案例 3：定位分布式执行瓶颈

某中台业务早晨出现用户操作卡顿，监控显示数据库集群中 `xx.xx.xx.x9` 节点 CPU 持续满载，导致 SQL 执行延迟飙升。DBA 尝试切换主节点和重启节点后，CPU 占用才恢复正常。
事后排查，由于故障期间的审计日志（SQL Audit）因重启丢失，我们通过 ASH（活动会话历史）数据分析故障时段（10:30 至 10:45）的会话状态，关键发现如下：

```shell
OceanBase(root@oceanbase)>select svr_ip, tenant_id, IN_REMOTE_DAS_EXECUTION, name.name as event, count(*) * 10 as cnt from CDB_WR_ACTIVE_SESSION_HISTORY ash left join v$event_name name on ash.event_no=name.`event#` where sample_time between "2024-11-29 10:30:45" and "2024-11-29 10:45:45" and tenant_id=1002 group by svr_ip, tenant_id, IN_REMOTE_DAS_EXECUTION, event order by cnt desc limit 20;
```

查询结果如下：

```shell
+---------------+-----------+-------------------------+--------------------------+------+
| svr_ip        | tenant_id | IN_REMOTE_DAS_EXECUTION | event                    | cnt  |
+---------------+-----------+-------------------------+--------------------------+------+
| xx.xx.xx.x9   |      1002 | Y                       |                          | 48330 |
| xx.xx.xx.x4   |      1002 | N                       | das wait remote response | 28920 |
| xx.xx.xx.x1   |      1002 | N                       | das wait remote response | 20050 |
| xx.xx.xx.x9   |      1002 | N                       | default condition wait   |  2460 |
| xx.xx.xx.x4   |      1002 | N                       | default condition wait   |  1450 |
| xx.xx.xx.x1   |      1002 | N                       | px loop condition wait   |  1420 |
| xx.xx.xx.x9   |      1002 | N                       | px loop condition wait   |  1330 |
| xx.xx.xx.x9   |      1002 | N                       |                          |  1170 |
| xx.xx.xx.x4   |      1002 | N                       |                          |   900 |
| xx.xx.xx.x1   |      1002 | N                       | default condition wait   |   830 |
| xx.xx.xx.x4   |      1002 | N                       | default condition wait   |   570 |
| xx.xx.xx.x4   |      1002 | N                       | px loop condition wait   |   530 |
| xx.xx.xx.x9   |      1002 | Y                       |                          |   520 |
+---------------+-----------+-------------------------+--------------------------+------+
```

关键发现如下：

| 节点IP | 关键阶段/等待事件 | 会话数量（占比）|
| ------ | --------------- | -------------- |
| xx.xx.xx.x9 | 远程 DAS 任务（IN_REMOTE_DAS_EXECUTION）| 48330 次（远超其他节点）|
| 其他节点 | 等待远程响应（das wait remote response）| 28920 次、20050 次 |

推导出以下结论：

1. 问题根源：

   * 故障节点（xx.xx.xx.x9）CPU 满载，是因为它同时处理了大量远程 DAS 请求（来自其他节点）。
   * 其他节点（如 xx.xx.xx.x4 和 xx.xx.xx.x1）因等待该节点的响应占用了大量工作线程，导致队列积压，进一步加剧了系统拥堵。

2. 深层原因：

   通过关联 SQL 维度分析，发现是两条定时执行的 AP 业务 SQL （如批量数据同步任务）在早晨触发，并发执行时消耗了过多 CPU 资源，挤占了正常业务 SQL 的资源。
