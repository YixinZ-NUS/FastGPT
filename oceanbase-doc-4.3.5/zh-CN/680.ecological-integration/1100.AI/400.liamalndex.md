|description|  |
|---|---|
|keywords| |
|dir-name|LlamaIndex|
|dir-name-en|LlamaIndex|
|tenant-type|MySQL Mode|

# OceanBase 数据库 Vector 与 LlamaIndex 集成

OceanBase 数据库提供了向量类型存储、向量索引、embedding 向量搜索的能力。可以将向量化后的数据存储在 OceanBase 数据库，供下一步的搜索使用。

LlamaIndex 是一个使用 LLM（包括代理和工作流）构建上下文增强生成 AI 应用程序的框架。它提供了数据连接器、数据索引、代理、可观察性/评估集成和工作流等能力。

本教程结合通义千问 API，演示如何将 OceanBase 数据库中的向量搜索功能、通义千问与 LlamaIndex 集成实现文档问答。

## 前提条件

* 您已完成部署 OceanBase 数据库 V4.3.3 及以上版本并且创建了 MySQL 模式租户。[创建租户](../../600.manage/200.tenant-management/600.common-tenant-operations/200.manage-create-tenant.md) 后，再参考下述步骤操作。

  * 您的环境中已存在可以使用的 MySQL 租户和 MySQL 数据库和账号，并已对对数据库账号授读写权限。
  * 确保您已经在租户中设置了 `ob_vector_memory_limit_percentage` 配置项，以启用向量搜索功能。V4.3.5 BP3 之前的版本推荐设置值为 `30`，从 V4.3.5 BP3 版本开始推荐保持默认值 `0`。如需更精确设置此配置项，请参考 [ob_vector_memory_limit_percentage](../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/400.tenant-level-configuration-items/6150.ob_vector_memory_limit_percentage.md) 计算此值。

* 安装 Python 3.9 及以上版本。
* 安装 所需的依赖项:

    ```shell
    python3 -m pip install llama-index-vector-stores-oceanbase llama-index
    python3 -m pip install llama-index-embeddings-dashscope
    python3 -m pip install llama-index-llms-dashscope
    ```

* 准备通义千问 API 密钥。

## 步骤一：获取数据库连接信息

联系 OceanBase 数据库部署人员或者管理员获取相应的数据库连接串，例如：

```sql
obclient -h$host -P$port -u$user_name -p$password -D$database_name
```

**参数说明：**

* `$host`：提供 OceanBase 数据库连接 IP。OceanBase 数据库代理（OceanBase Database Proxy，ODP）连接方式使用的是一个 ODP 地址；直连方式使用的是 OBServer 节点的 IP 地址。
* `$port`：提供 OceanBase 数据库连接端口。ODP 连接的方式默认是 `2883`，在部署 ODP 时可自定义；直连方式默认是 `2881`，在部署 OceanBase 数据库时可自定义。
* `$database_name`：需要访问的数据库名称。

    <main id="notice" type='notice'>
        <h4>注意</h4>
        <p>连接租户的用户需要拥有该数据库的 <code>CREATE</code>、<code>INSERT</code>、<code>DROP</code> 和 <code>SELECT</code> 权限。更多有关用户权限的信息，请参见 <a href="../../600.manage/500.security-and-permissions/300.access-control/200.user-and-permission/200.permission-of-mysql-mode/100.permission-classification-of-mysql.md">MySQL 模式下的权限分类</a>。</p>
    </main>

* `$user_name`：提供租户的连接账户。ODP 连接的常用格式：`用户名@租户名#集群名` 或者 `集群名:租户名:用户名`；直连方式格式：`用户名@租户名`。
* `$password`：提供账户密码。

更多连接串的信息，请参见 [通过 OBClient 连接 OceanBase 租户](../../300.develop/100.application-development-of-mysql-mode/100.connect-to-oceanbase-database-of-mysql-mode/300.connect-to-an-oceanbase-tenant-by-using-obclient-of-mysql-mode.md)。

## 步骤二：构建您的 AI 助手

### 设置通义千问 API key 环境变量

获取 [通义千问 API 密钥](https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key) 并 [配置API-KEY到环境变量](https://help.aliyun.com/zh/model-studio/developer-reference/configure-api-key-through-environment-variables)。

```shell
export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_API_KEY"
```

### 下载示例数据

```shell
mkdir -p '/root/llamaindex/paul_graham/'
wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O '/root/llamaindex/paul_graham/paul_graham_essay.txt'
```

### 加载数据文本

```python
import os
from pyobvector import ObVecClient
from llama_index.core import Settings
from llama_index.embeddings.dashscope import DashScopeEmbedding
from llama_index.core import (
SimpleDirectoryReader,
load_index_from_storage,
VectorStoreIndex,
StorageContext,
)
from llama_index.vector_stores.oceanbase import OceanBaseVectorStore
from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels
#set ob client
client = ObVecClient(uri="127.0.0.1:2881", user="root@test",password="",db_name="test")
# Global Settings
Settings.embed_model = DashScopeEmbedding()
# config llm model
dashscope_llm = DashScope(
    model_name=DashScopeGenerationModels.QWEN_MAX,
    api_key=os.environ.get("DASHSCOPE_API_KEY", ""),
)
# load documents
documents = SimpleDirectoryReader("/root/llamaindex/paul_graham/").load_data()
oceanbase = OceanBaseVectorStore(
    client=client,
    dim=1536,
    drop_old=True,
    normalize=True,
)

storage_context = StorageContext.from_defaults(vector_store=oceanbase)
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context
)
```

## 向量搜索

此步骤演示如何从文档 `paul_graham_essay.txt` 中查询 `“What did the author do growing up?”`。

```shell
# set Logging to DEBUG for more detailed outputs
query_engine = index.as_query_engine(llm=dashscope_llm)
res = query_engine.query("What did the author do growing up?")
res.response
```

预期结果：

```python
'Growing up, the author worked on writing and programming outside of school. In terms of writing, he wrote short stories, which he now considers to be awful, as they had very little plot and focused mainly on characters with strong feelings. For programming, he started in 9th grade by trying to write programs on an IBM 1401 at his school, using an early version of Fortran. Later, after getting a TRS-80 microcomputer, he began to write more practical programs, including simple games, a program to predict the flight height of model rockets, and a word processor that his father used for writing.'
```
