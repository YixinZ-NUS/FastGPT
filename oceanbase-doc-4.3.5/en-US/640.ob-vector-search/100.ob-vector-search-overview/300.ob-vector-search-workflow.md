| Description   |                 |
|---------------|-----------------|
| keywords      |                 |
| dir-name      |                 |
| dir-name-en   |                 |
| tenant-type   | MySQL Mode      |

# AI application workflow with OceanBase vector search

This topic describes the AI application workflow with OceanBase vector search.

<!-- ## AI application workflow

![AI application workflow](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer/V4.3.5/vector_search/AI-Workflow1.jpg)

This architecture diagram illustrates a typical modern AI application pattern that combines the capabilities of traditional relational databases with advanced vector search and large language model (LLM) technologies. This integration enables intelligent processing and efficient utilization of multimodal data. OceanBase Database plays a crucial role in this workflow, providing robust data storage and vector search capabilities, which form the foundation for building such AI applications. -->

## Core steps

**1. Convert unstructured data into feature vectors using vector embedding**

The workflow begins with unstructured data, such as videos, documents, and images. These diverse forms of unstructured data are converted into vector representations through vector embedding models. The task of these models is to transform raw, complex unstructured data into high-dimensional vectors. These vectors capture the semantic information and features of the data, and their similarity can be expressed through distances in the vector space. For more information, see [Vector embedding technology](../150.ob-vector-search-generate-vector-embeddings.md).

**2. Store vector embeddings and create vector indexes in OceanBase Database**

As the core storage layer, OceanBase Database stores all types of data, including traditional relational tables for business data, raw unstructured data, and vector data generated from vector embeddings. For more information, see [Store vector data](../160.ob-vector-search-store-vector-embeddings.md).

To enable efficient vector search, OceanBase Database builds vector indexes internally for vector data. A vector index is a specialized data structure that significantly accelerates nearest neighbor search in high-dimensional vector spaces. Since computing vector similarity is computationally intensive, exact search (which calculates the distance for every vector) ensures accuracy but results in a significant drop in query performance. With vector indexes, the system can quickly locate candidate vectors, greatly reducing the number of vectors that need distance calculations while maintaining high accuracy and significantly improving query efficiency. For more information, see [Create a vector index](../200.ob-vector-index.md).

**3. Perform nearest neighbor search and hybrid search using SQL or SDK**

Users interact with the AI application through clients or programming languages, submitting queries that may involve text, images, or other data types. For more information, see [Supported clients and languages](../700.ob-vector-search-reference/900.ob-vector-search-supported-clients-and-languages/100.ob-vector-search-supported-clients-and-languages-overview.md).

OceanBase Database uses SQL statements to query and manage relational data, enabling hybrid search for both scalar and vector data. When a user initiates a query, if it's an unstructured query, the system first converts it into a vector using the vector embedding model. Then, the system leverages vector indexes and scalar indexes to quickly search for vectors that are most similar to the query vector and meet the scalar filtering conditions. This process efficiently retrieves the most relevant unstructured data. For more information about nearest neighbor search, see [Nearest neighbor search](../300.ob-vector-similarity-search.md).

**4. Generate prompts and submit them to the LLM for inference**

In the final stage, the system generates optimized prompts based on the results of the hybrid search and submits them to the large language model (LLM) for complete inference. The LLM generates natural language responses based on these contextual inputs. There is a feedback loop between the LLM and the vector embedding model, meaning that the LLM's outputs or user feedback can be used to optimize the vector embedding model, creating a continuous learning and optimization cycle.