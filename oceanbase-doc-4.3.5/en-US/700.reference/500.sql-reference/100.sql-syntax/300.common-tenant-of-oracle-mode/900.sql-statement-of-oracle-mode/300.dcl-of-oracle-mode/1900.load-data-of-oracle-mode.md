| Description   |                 |
|---------------|-----------------|
| keywords      |                 |
| dir-name      |                 |
| dir-name-en   |                 |
| tenant-type   | Oracle Mode     |
|machine-translation||

# LOAD DATA

## Purpose

* **The `LOAD DATA` statement of OceanBase Database supports the following input files:**

  * **Server-side (OBServer node) files**: Files located on the OBServer nodes of OceanBase Database. You can use the `LOAD DATA INFILE` statement or the `LOAD DATA FROM URL` statement to load data from server-side files into database tables.

  * **Client-side (local) files**: Files located in the local file system of the client. You can use the `LOAD DATA LOCAL INFILE` statement or the `LOAD DATA FROM URL` statement to load data from client-side local files into database tables.

    <main id="notice" type='explain'>
      <h4>Note</h4>
      <p>When executing the <code>LOAD DATA LOCAL INFILE</code> command, OceanBase Database automatically adds the <code>IGNORE</code> option.</p>
    </main>

  * **OSS files**: Files stored in an OSS file system. You can use the `LOAD DATA REMOTE_OSS INFILE` statement to load data from OSS files to a database table.

* **`LOAD DATA` currently supports importing text files in CSV format. The entire import process can be divided into the following steps:**

  1. **File parsing**: OceanBase Database reads the data from the file specified by the user and determines whether to parse the input file data in parallel or serially based on the specified degree of parallelism.

  2. **Data distribution**: Since OceanBase Database is a distributed database, data for different partitions may be distributed across different OBServer nodes. `LOAD DATA` calculates the parsed data and determines which OBServer node the data should be sent to.

  3. **Data insertion**: Once the target OBServer node receives the data, it performs the `INSERT` operation locally to insert the data into the corresponding partition.

## Considerations

* Tables with triggers are not allowed to use the <code>LOAD DATA</code> statement.
* To import data from external files, you need to have the <code>FILE</code> privilege and the following configurations:

  * When loading server-side files, you need to set the system variable <a href="../../../../../800.configuration-items-and-system-variables/200.system-variable/300.global-system-variable/12000.secure_file_priv-global.md">secure_file_priv</a> in advance to configure the accessible path for importing or exporting files.
  * When loading client-side local files, you need to add the <code>--local-infile[=1]</code> option when starting the MySQL/[OBClient](https://www.oceanbase.com/softwarecenter-cloud) client to enable the functionality of loading data from the local file system.

* When using specified partition bypass import, note that the target table cannot be a replicated table, and it must not contain auto-increment columns, identity columns, or a Global Index.

To import data from an external file, you must have the `FILE` privilege. You can execute the `GRANT FILE ON *.* TO $user_name;` statement to grant the privilege to the user specified by `$user_name`, who needs to execute the `LOAD DATA` statement.

## Syntax

```sql
-- Load data from a regular file.
LOAD DATA
    [/*+ PARALLEL(N) [load_batch_size(M)] [APPEND | direct(bool, int, [load_mode])] | NO_DIRECT */]
    [REMOTE_OSS | LOCAL] INFILE 'file_name'
    [REPLACE | IGNORE]
    INTO TABLE table_name [PARTITION(PARTITION_OPTION)] 
    [COMPRESSION [=] {AUTO|NONE|GZIP|DEFLATE|ZSTD}]
    [{FIELDS | COLUMNS}
        [TERMINATED BY 'string']
        [[OPTIONALLY] ENCLOSED BY 'char']
        [ESCAPED BY 'char']
    ]
    [LINES
        [STARTING BY 'string']
        [TERMINATED BY 'string']
    ]
    [IGNORE number {LINES | ROWS}]
    [(column_name_var
        [, column_name_var] ...)]

load_mode:
    'full' 
    | 'inc_replace'

PARTITION_OPTION:
    partition_option_list
    | subpartition_option_list

-- Load data from a URL.
LOAD DATA
    [/*+ PARALLEL(N) [load_batch_size(M)] [APPEND | direct(bool, int, [load_mode])] | NO_DIRECT */]
    [REPLACE | IGNORE]
    FROM { url_table_function_expr |
         ( SELECT expression_list FROM url_table_function_expr ) }
    INTO TABLE table_name
    [PARTITION(PARTITION_OPTION)] 
    [(column_name_var [, column_name_var] ...)]
    [LOG ERRORS [REJECT LIMIT {integer | UNLIMITED}]]

load_mode:
    'full' 
    | 'inc_replace'

url_table_function_expr:

  | FILES (
    LOCATION = '<string>',
    {
      FORMAT = (
        TYPE = 'CSV',
        LINE_DELIMITER = '<string>' | <expr>,
        FIELD_DELIMITER = '<string>' | <expr>,
        PARSE_HEADER = { TRUE | FALSE },
        ESCAPE = '<character>' | <expr>,
        FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | <expr>,
        ENCODING = 'charset',
        NULL_IF = ('<string>' | <expr>, '<string>' | <expr> ...),
        SKIP_HEADER = <int>,
        SKIP_BLANK_LINES = { TRUE | FALSE },
        TRIM_SPACE = { TRUE | FALSE },
        EMPTY_FIELD_AS_NULL = { TRUE | FALSE }
      )
      | FORMAT = ( TYPE = 'PARQUET' | 'ORC' )
    },
    [PATTERN = '<regex_pattern>']
  )
  | SOURCE (
      TYPE = 'ODPS',
      ACCESSID = '<string>',
      ACCESSKEY = '<string>',
      ENDPOINT = '<string>',
      TUNNEL_ENDPOINT = '<string>',
      PROJECT_NAME = '<string>',
      SCHEMA_NAME = '<string>',
      TABLE_NAME = '<string>',
      QUOTA_NAME = '<string>',
      COMPRESSION_CODE = '<string>'
    )

PARTITION_OPTION:
    partition_option_list
    | subpartition_option_list

```

## Parameters

| Parameter | Description |
|-----------|-------------|
| parallel(N)                      | The degree of parallelism for loading data, with a default value of 4.   |
| load_batch_size(M)               | Specifies the batch size for each insertion. The default value of `M` is `100`. The recommended range is [100, 1000].    |
| APPEND \| direct() \|NO_DIRECT   | Uses hints to enable direct load.<main id="notice" type='notice'><h4>Notice</h4><p>During a direct load task, we recommend that you do not upgrade OceanBase Database, because this may cause the direct load task to fail. </p></main> <ul><li>The `APPEND` hint is equivalent to using `direct(true, 0)` by default. It also allows you to collect statistics online (`GATHER_OPTIMIZER_STATISTICS` hint). </li><li>The `direct()` parameter is described as follows:<ul><li><code>bool</code>: specifies whether to sort the data to be written. The value <code>true</code> specifies to sort the data, and the value <code>false</code> specifies not to sort the data. </li><li><code>int</code>: specifies the maximum number of tolerated error rows. </li><li><code>load_mode</code>: an optional parameter that specifies the direct load mode. The value must be enclosed in single quotation marks (' ') and can be one of the following:<ul><li><code>full</code>: specifies a full direct load. This is the default value. </li><li><code>inc</code>: specifies an incremental direct load that supports the <code>INSERT</code> and <code>IGNORE</code> semantics. </li><li><code>inc_replace</code>: specifies an incremental direct load that does not check for duplicate primary keys. This is equivalent to an incremental direct load with the <code>REPLACE</code> semantics. </li></ul></li></ul></li><li><code>NO_DIRECT</code>: a hint that controls the execution of a single SQL statement to bypass direct load. If this hint is specified in an SQL statement, other hints for direct load in the statement are ignored, and the SQL statement is executed as a normal load. </li></ul> For more information about using the `LOAD DATA` statement for direct load, see [Import data or files by using the LOAD DATA statement](../../../../../../620.obap/300.obap-import-data/20.bypass-import/200.full-bypass-import.md). |
| REMOTE_OSS \| LOCAL | Optional. Specifies whether to read data from an object storage service or the local file system of the client.<ul><li> <code>REMOTE_OSS</code> specifies whether to read data from an object storage service. Currently, Alibaba Cloud OSS, Amazon S3, and object storage services that are compatible with S3 such as OBS, GCS, and COS are supported. </li><li> <code>LOCAL</code> specifies whether to read data from the local file system of the client. </li><li>If you do not specify <code>REMOTE_OSS</code> or <code>LOCAL</code>, data in the file system of the OBServer node is imported. </li></ul> |
| file_name           | The path and name of the input file. The format of the <code>file_name</code> parameter corresponds to the type of <code>REMOTE_OSS \| LOCAL</code>:<ul><li>When you specify <code>REMOTE_OSS</code>, the value of <code>file_name</code> indicates the path and name of the file in the object storage service. The following examples show the paths and names of files in different object storage services:<ul><li><b>Alibaba Cloud OSS</b>: <code>LOAD DATA /\*+ [APPEND \| DIRECT(need_sort,max_error)] parallel(N) \*/ REMOTE_OSS INFILE 'oss://\$PATH/\$FILENAME/?host=\$HOST&access_id=\$ACCESS_ID&access_key=\$ACCESS_KEY' INTO TABLE table_name ...;</code></li><li><b>Amazon S3</b>: <code>LOAD DATA /\*+ [APPEND \| DIRECT(need_sort,max_error)] parallel(N) \*/ REMOTE_OSS INFILE 's3://\$PATH/\$FILENAME/?host=\$HOST&access_id=\$ACCESS_ID&access_key=\$ACCESS_KEY&s3_region=S3_REGION' INTO TABLE table_name ...;</code></li><li><b>Object storage services that are compatible with S3</b>: <code>LOAD DATA /\*+ [APPEND \| DIRECT(need_sort,max_error)] parallel(N) \*/ REMOTE_OSS INFILE 's3://\$PATH/\$FILENAME/?host=\$HOST&access_id=\$ACCESS_ID&access_key=\$ACCESS_KEY' INTO TABLE table_name ...;</code></li></ul></li><li>When you specify <code>LOCAL</code>, the value of <code>file_name</code> indicates the path and name of the file in the local file system of the client. The syntax is <code>LOAD DATA /*+ [APPEND \| DIRECT(need_sort,max_error)] parallel(N) */ INFILE '/\$PATH/\$FILENAME' INTO TABLE table_name ...;</code>. </li><li>If you do not specify <code>REMOTE_OSS</code> or <code>LOCAL</code>, the syntax is the same as that when you specify <code>LOCAL</code>. </li></ul>The following table describes the parameters in the syntax. <ul><li><code>\$PATH</code>: the path of the file in the bucket, which indicates the directory where the file is located. </li><li><code>\$FILENAME</code>: the name of the file, which indicates the specific file to be accessed. </li><li><code>\$HOST</code>: the host name or domain name of the server where the OSS service is located or the server where the CDN is accelerated. It indicates the address where the OSS service is located. </li><li><code>\$ACCESS_ID</code>: the Access Key ID required for accessing the OSS service. It is used for authentication. </li><li><code>\$ACCESSKEY</code>: the Access Key Secret required for accessing the OSS service. It is used for authentication. </li><li><code>S3_REGION</code>: the region where the Amazon S3 bucket is located. It is a required parameter when the object storage service is Amazon S3. </li></ul><main id="notice" type='explain'><h4>Note</h4> <p>When you import a file from OSS, make sure of the following:<ul><li>You have the permissions to access the OSS bucket and file. You must have sufficient permissions to read the specified bucket and file. This usually requires that you set the permissions in the OSS console or by using OSS APIs and configure the access keys (Access Key ID and Access Key Secret) as credentials with appropriate permissions. </li><li>The database server can connect to the specified <code>$HOST</code> over the network to access the OSS service. If the <code>$HOST</code> is a domain name accelerated with CDN, the CDN is correctly configured and the network connection is normal. </li></ul></p> </main>  |
| table_name                       | The name of the table to which the imported data belongs. You can specify partitioned or non-partitioned tables.  |
| PARTITION_OPTION                   | The name of the partition for direct load. You can specify multiple partitions by separating them with commas (,).<ul><li>partition_option_list: a list of partition names for direct load. </li> <li>subpartition_option_list: a list of subpartition names for direct load. </li></ul><main id="notice" type='explain'><h4>Note</h4><p>You can import data only by using direct load to a specified partition. Normal LOAD DATA does not support specifying partitions. If you do not specify direct load hints or parameters, specifying partitions in the LOAD DATA statement is invalid. </p></main>|
| COMPRESSION                      | The compression format of the file. The valid values are described as follows, where<ul><li><code>AUTO</code>: automatically detects the compression algorithm based on the file name suffix. <br> When you use the <code>AUTO</code> parameter, different suffixes correspond to specific compression formats, as described in the following table. <ul><li><code>.gz</code>: a GZIP-compressed file. </li><li><code>.deflate</code>: a DEFLATE-compressed file. </li><li><code>.zst/.zstd</code>: a ZSTD-compressed file. </li></ul></li><li><code>NONE</code>: indicates that the file is not compressed. </li><li><code>GZIP</code>: a GZIP-compressed file. </li><li><code>DEFLATE</code>: a DEFLATE-compressed file without metadata. </li><li><code>ZSTD</code>: a ZSTD-compressed file. </li></ul> You can explicitly specify the compression format of a file or let the system detect the compression format based on the file name suffix. |
| FIELDS \| COLUMNS                | The format of fields. <ul><li> `ENCLOSED BY`: the delimiter that encloses the exported values.  </li> <li> `TERMINATED BY`: the delimiter that terminates the exported columns. </li> <li>`ESCAPED BY`: the character that is ignored in exported values. </li></ul>    |
| LINES STARTING BY                | The delimiter that indicates the start of a line.   |
| LINES TERMINATED BY              | The delimiter that indicates the end of a line.  |
| IGNORE number { LINES \| ROWS } | The number of the first few lines to be ignored. `LINES` indicates the first few lines of the file, and `ROWS` indicates the first few rows separated by the field delimiter. By default, the system matches each input file column with a table column. If the input file does not contain all the columns in the table, the system fills in the missing columns based on the following rules:  <ul><li> Character type: an empty string.   </li> <li> Numeric type: 0.    </li> <li> Date type: `0000-00-00`.</li></ul><main id="notice" type='explain'><h4>Note</h4><p>The behavior is the same for multi-file import as it is for single-file import. </p></main>    |
| column_name_var                  | The name of the column to which the imported data belongs.  |
| LOG ERRORS [REJECT LIMIT {integer \| UNLIMITED}] | Optional. Enables error diagnostics during the process of importing URL external tables. For detailed information, refer to [log_errors](#log_errors) below.<main id="notice" type='explain'><h4>Note</h4><p>For OceanBase Database V4.3.5, the syntax for importing URL external tables with the LOAD DATA statement supports specifying error diagnostics starting from V4.3.5 BP2.</p></main> |

### FILES

The `FILES` keyword consists of the clauses `LOCATION`, `FORMAT`, and `PATTERN`.

* The `LOCATION` clause is used to specify the path where the external table files are stored. Typically, the data files for the external table are stored in a single directory. The directory can include subdirectories, and when the table is created, the external table will automatically collect all files in that directory.

  * The local `LOCATION` format is `LOCATION = '[file://] local_file_path'`, where `local_file_path` can be either a relative path or an absolute path. If a relative path is specified, the current directory must be the installation directory of the OceanBase database. The `secure_file_priv` parameter is used to configure the file paths that OBServer nodes have permission to access. The `local_file_path` must be a subpath of the `secure_file_priv` path.

  * The remote `LOCATION` format is as follows:  
    * `LOCATION = '{oss|cos|S3}://$ACCESS_ID:$ACCESS_KEY@$HOST:s3_region/remote_file_path'`, where `$ACCESS_ID`, `$ACCESS_KEY`, and `$HOST` are the access credentials required to connect to OSS, COS, or S3. The `s3_region` specifies the region information for S3. These sensitive access credentials are stored in the database's system tables in an encrypted format.  
    * `LOCATION = 'hdfs://${hdfs_namenode_address}:${port}/PATH.localhost'`, where `port` refers to the port number of HDFS, and `PATH` specifies the directory path in HDFS.  
      * With Kerberos authentication: `LOCATION = 'hdfs://localhost:port/user?principal=xxx&keytab=xxx&krb5conf=xxx&configs=xxx'`.
        Where:
        * `principal`: the authenticated user.  
        * `keytab`: the key file for user authentication.  
        * `krb5conf`: the Kerberos environment description file used by the user.  
        * `configs`: additional HDFS configuration options. By default, this is empty. However, in a Kerberos environment, this configuration typically needs to be set. For example, `dfs.data.transfer.protection=authentication,privacy`, which specifies the data transfer protection level as `authentication` and `privacy`.

    <main id="notice" type='notice'>
      <h4>Note</h4>
      <p>When using an object storage path, the parameters of the object storage path are separated by the <code>&</code> symbol. Please ensure that the parameter values you enter only contain uppercase and lowercase English letters, numbers, <code>\/-_$+=</code>, and wildcards. If you enter any characters other than those mentioned above, it may cause the configuration to fail.</p>
    </main>

* The `FORMAT` clause specifies attributes related to the file format. Valid values: CSV, PARQUET, and ORC.

  * When **TYPE = 'CSV'**, the following fields are included:

  * `LINE_DELIMITER`: the line delimiter in the CSV file. The default value is `LINE_DELIMITER='\n'`.  
  * `FIELD_DELIMITER`: optional. Specifies the column delimiter in the CSV file. The default value is `FIELD_DELIMITER='\t'`.  
  * `PARSE_HEADER`: optional. Specifies whether the first line of the CSV file is the column name for each column. The default value is `FALSE`, meaning the first line of the CSV file is not treated as column names.  
  * `ESCAPE`: the escape character in the CSV file. It must be a single byte. The default value is `ESCAPE='\'`.  
  * `FIELD_OPTIONALLY_ENCLOSED_BY`: optional. Specifies the symbol used to enclose field values in the CSV file. The default value is empty. Using this option means that only certain types of fields (such as CHAR, VARCHAR, TEXT, and JSON) will be enclosed.  
  * `ENCODING`: the character set encoding format of the file. If not specified, the default value is `UTF8MB4`.  
  * `NULL_IF`: the string that should be treated as `NULL`. The default value is empty.  
  * `SKIP_HEADER`: skips the file header and specifies the number of lines to skip.  
  * `SKIP_BLANK_LINES`: specifies whether to skip blank lines. The default value is `FALSE`, meaning blank lines are not skipped.  
  * `TRIM_SPACE`: specifies whether to remove leading and trailing spaces from fields in the file. The default value is `FALSE`, meaning leading and trailing spaces are not removed.  
  * `EMPTY_FIELD_AS_NULL`: specifies whether to treat empty strings as `NULL`. The default value is `FALSE`, meaning empty strings are not treated as `NULL`.  

* When **TYPE = 'PARQUET/ORC'**, there are no additional fields.

* The `PATTERN` clause specifies a regular pattern string for filtering files in the directory specified by the `LOCATION` clause. For each file in the directory specified by the `LOCATION` clause, if the file path matches the pattern string, the external table accesses the file. Otherwise, the external table skips the file. If this parameter is not specified, the external table accesses all files in the directory specified by the `LOCATION` clause by default.

### SOURCE

The `SOURCE` clause takes no other clauses and contains the following fields when **TYPE = 'ODPS'**:

* `ACCESSID`: specifies the ID of the ODPS user.
* `ACCESSKEY`: specifies the password of the ODPS user.
* `ENDPOINT`: specifies the connection address of the ODPS service.
* `TUNNEL_ENDPOINT`: specifies the connection address of the Tunnel data transmission service.
* `PROJECT_NAME`: specifies the project in which the table to be queried is located.
* `SCHEMA_NAME`: optional. Specifies the schema in which the table to be queried is located.
* `TABLE_NAME`: specifies the name of the table to be queried.
* `QUOTA_NAME`: optional. Specifies whether to use the specified quota.
* `COMPRESSION_CODE`: optional. Specifies the compression format of the data source. Valid values: `ZLIB`, `ZSTD`, `LZ4`, and `ODPS_LZ4`. The default value is an empty string, which specifies not to enable compression.

### log_errors

* `LOG ERRORS`: Indicates that error rows will be skipped during the import process.

* `REJECT LIMIT`: Optional, used to set the maximum allowable number of error rows:

  * Default value is 0: This means no error rows are allowed, and the operation will fail upon encountering the first error.
  * `integer`: Specifies the maximum number of allowable error rows per machine. For example, 10 means a maximum of 10 error rows per machine.
  * `UNLIMITED`: Allows an unlimited number of error rows.

<main id="notice" type='notice'>
  <h4>Notice</h4>
  <p><ul><li>If the <code>LOG ERRORS</code> clause is not specified, the behavior will follow the standard import process, meaning the operation will fail immediately upon encountering the first error.</li><li>If the <code>LOG ERRORS</code> clause is specified but the <code>REJECT LIMIT</code> clause is not specified, it is equivalent to setting the <code>LIMIT</code> to 0 for diagnostics. In this case, the operation will fail upon encountering the first error, but the first encountered error will be logged, and the error code will indicate a diagnostic-related error, such as "reject limit reached".</li></ul></p>
</main>

### Wildcard rules for direct load of multiple files

To facilitate the import of multiple files, the wildcard feature is introduced for server-side and OSS file imports, but not for client-side file imports.

* **Use of wildcards on the server**

  * **Matching rules:**

    * Matching filenames: `load data /*+ parallel(20) direct(true, 0) */ infile '/xxx/test.*.csv' replace into table t1 fields terminated by '|';`

    * Matching directories: `load data /*+ parallel(20) direct(true, 0) */ infile '/aaa*bb/test.1.csv' replace into table t1 fields terminated by '|';`

    * Matching both directories and filenames: `load data /*+ parallel(20) direct(true, 0) */ infile '/aaa*bb/test.*.csv' replace into table t1 fields terminated by '|';`

  * **Considerations:**

    * At least one matching file must exist. Otherwise, an error code 4027 is returned.

    * For `load data /*+ parallel(20) direct(true, 0) */ infile '/xxx/test.1*.csv,/xxx/test.6*.csv' replace into table t1 fields terminated by '|';`, `/xxx/test.1*.csv,/xxx/test.6*.csv` will be considered as a whole for matching. If no file matches, an error code 4027 will be returned.

    * Only wildcards supported by the POSIX GLOB function can be used. For example, `test.6*(6|0).csv` and `test.6*({0.csv,6.csv}|.csv)` are allowed. Although these wildcards can be used, they will not match any result, and an error code 4027 will be returned when you use them.

* **Use of wildcards in object storage service (OSS)**

  * **Matching rules:**

    Matching filenames: `load data /*+ parallel(20) direct(true, 0) */ remote_oss infile 'oss://xxx/test.*.csv?host=xxx&access_id=xxx&access_key=xxx' replace into table t1 fields terminated by '|';`

  * **Considerations:**

    * Directory matching is not supported. For example, `load data /*+ parallel(20) direct(true, 0) */ remote_oss infile 'oss://aa*bb/test.*.csv?host=xxx&access_id=xxx&access_key=xxx' replace into table t1 fields terminated by '|';` will return `OB_NOT_SUPPORTED`.

    * Only `*` and `?` are supported as wildcard characters for filenames. Other wildcard characters can be entered but will not match any result.

## Examples

### Import data from a server

**Example 1: Import data from a server.**

1. Set the global secure path.

   <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>For security reasons, when you set the system variable <code>secure_file_priv</code>, you can connect to the database only through a local socket to execute the SQL statement that modifies the global variable. For more information, see <a href="../../../../../800.configuration-items-and-system-variables/200.system-variable/300.global-system-variable/12000.secure_file_priv-global.md">secure_file_priv</a>.</p>
   </main>

   ```shell
   obclient> SET GLOBAL secure_file_priv = "/";
   ```

2. Log out.

   <main id="notice" type='explain'>
     <h4>Note</h4>
     <p>Since <code>secure_file_priv</code> is a <code>GLOBAL</code> variable, you need to execute <code>\q</code> to make the setting take effect.</p>
   </main>

   ```shell
   obclient> \q
   ```

   The return result is as follows:

   ```shell
   Bye
   ```

3. Reconnect to the database and use the `LOAD DATA` statement to import data.

   * Perform a regular import.

      ```shell
      obclient> LOAD DATA INFILE '/home/admin/test.csv' INTO TABLE t1;
      ```

   * Use the `APPEND` hint to enable direct load.

      ```shell
      LOAD DATA /*+ PARALLEL(4) APPEND */ INFILE '/home/admin/test.csv' INTO TABLE t1;
      ```

**Example 2: Use the `APPEND` hint to enable direct load.**

```shell
LOAD DATA /*+ PARALLEL(4) APPEND */
   INFILE '/home/admin/a.csv'
   INTO TABLE t;
```

**Example 3: Import a CSV file.**

  * Import all columns from the `test1.csv` file.

    ```shell
    load data  /*+ direct(true,0) parallel(2)*/
    from files(
      location = "data/csv",
      format = (
        type = 'csv',
        field_delimiter = ',',
        parse_header = true,
        skip_blank_lines = true
      ),
      pattern = 'test1.csv')
    into table t1;
    ```

  * Read the `c1` and `c2` columns from the `test1.csv` file in the `data/csv` directory and import them into the `col1` and `col2` columns of the `t1` table.

    ```shell
    load data  /*+ direct(true,0) parallel(2)*/
    from (
      select c1, c2 from files(
          location = 'data/csv'
            format = (
            type = 'csv',
            field_delimiter = ',',
            parse_header = true,
            skip_blank_lines = true
          ),
          pattern = 'test1.csv'))
    into table t1 (col1, col2);
    ```

**Example 4: Import a PARQUET file.**

```shell
load data  /*+ direct(true,0) parallel(2)*/
from files(
  location = "data/parquet",
  format = ( type = 'PARQUET'),
  pattern = 'test1.parquet')
into table t1;
```

**Example 5: Import an ORC file.**

```shell
load data  /*+ direct(true,0) parallel(2)*/
from files(
  location = "data/orc",
  format = ( type = 'ORC'),
  pattern = 'test1.orc')
into table t1;
```

**Example 5: Import an ODPS file.**

```shell
load data  /*+ direct(true,0) parallel(2)*/
from source (
  type = 'ODPS',
  accessid = '$ODPS_ACCESSID',
  accesskey = '******',
  endpoint= '$ODPS_ENDPOINT',
  project_name = 'example_project',
  schema_name = '',
  table_name = 'example_table',
  quota_name = '',
  compression_code = '')
into table t1;
```

### Import data from a local file

**Example 1: Import data from a local file to a table in OceanBase Database.**

1. Open the terminal or command prompt window and enter the following command to start the client.

   ```shell
   obclient --local-infile -hxxx.xxx.xxx.xxx -P2881 -usys@oracle001 -p******
   ```

   The return result is as follows:

   ```shell
   Welcome to the OceanBase.  Commands end with ; or \g.
   Your OceanBase connection id is 3221548006
   Server version: OceanBase 4.2.2.0 (r100000032024010510-75c47d4be18a399e13c5309de1a81da5caf4e7c0) (Built Jan  5 2024 10:17:55)

   Copyright (c) 2000, 2018, OceanBase and/or its affiliates. All rights reserved.

   Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

   obclient [SYS]>
   ```

    <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>To use the <code>LOAD DATA LOCAL INFILE</code> feature, you must use OBClient V2.2.4 or later.</p>
    </main>

2. In the client, execute the `LOAD DATA LOCAL INFILE` statement to load data from the local file.

   ```shell
   obclient [SYS]> LOAD DATA LOCAL INFILE '/home/admin/test_data/tbl1.csv' INTO TABLE tbl1 FIELDS TERMINATED BY ',';
   ```

   The return result is as follows:

   ```shell
   Query OK, 3 rows affected
   Records: 3  Deleted: 0  Skipped: 0  Warnings: 0
   ```

**Example 2: Directly import a compressed file by setting COMPRESSION.**

```shell
LOAD DATA LOCAL INFILE '/your/file/lineitem.tbl.gz' 
    INTO TABLE lineitem 
    COMPRESSION GZIP 
    FIELDS TERMINATED BY '|';
```

**Example 3: Specify a partition for direct load.**

* Specify a partition by using the partitioning key.

```shell
load data /*+ direct(true,0) parallel(2) load_batch_size(100) */ 
infile "$FILE_PATH" 
into table t1 partition(p0, p1) 
fields terminated by '|' enclosed by '' lines starting by '' terminated by '\n';
```

* Specify a subpartition by using the subpartitioning key.

```shell
load data /*+ direct(true,0) parallel(2) load_batch_size(100) */ 
infile "$FILE_PATH" 
into table t1 partition(p0sp0, p1sp1) 
fields terminated by '|' enclosed by '' lines starting by '' terminated by '\n';
```

### Import data from an OSS file

**Example 1: Use the `direct(bool, int)` hint to enable direct load, where the file to be loaded is stored in OSS.**

```shell
LOAD DATA /*+ direct(true,1024) parallel(16) */ REMOTE_OSS INFILE 'oss://antsys-oceanbasebackup/backup_rd/xiaotao.ht/lineitem2.tbl?host=***.oss-cdn.***&access_id=***&access_key=***' INTO TABLE tbl1 FIELDS TERMINATED BY ',';
```

### Import data from server-side files as URL external tables

<main id="notice" type='notice'>
  <h4>Notice</h4>
  <p>The commands involving IP addresses in the examples have been anonymized. Replace the IP addresses with the actual ones when you verify the commands.</p>
</main>

The following steps illustrate how to create external tables in the Oracle-compatible mode of OceanBase Database with external files located locally:

1. Prepare the external file.

    Execute the following command to create the file `column_conv.csv` in the `/home/admin/test_csv` directory on the machine where you will log into the OBServer node.

    ```shell
    [admin@xxx /home/admin/external_csv]# vi column_conv.csv
    ```

    The content of the file is as follows:

    ```shell
    1,short,short
    2,long_text,long_text
    3,long_text,long_text
    ```

2. Set the file path for import.

    <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>For security reasons, when setting the system variable <code>secure_file_priv</code>, you can only modify this global variable via a local Unix Socket connection to the database. For more information, see <a href="../../../../../800.configuration-items-and-system-variables/200.system-variable/300.global-system-variable/12000.secure_file_priv-global.md">secure_file_priv</a>.</p>
    </main>

    1. Execute the following command to log in to the machine where the OBServer node is located.

        ```shell
        ssh admin@10.10.10.1
        ```

    2. Execute the following command to connect to the tenant `oracle001` using a local Unix Socket connection.

        ```shell
        obclient -S /home/admin/oceanbase/run/sql.sock -usys@oracle001 -p******
        ```

    3. Execute the following SQL command to set the import path to `/home/admin/test_csv`.

        ```sql
        SET GLOBAL secure_file_priv = "/home/admin/test_csv";
        ```

3. Reconnect to tenant `oracle001`.

    Example:

    ```shell
    obclient -h10.10.10.1 -P2881 -usys@oracle001 -p****** -A
    ```

4. Create the table `test_tbl1`.

    ```sql
    CREATE TABLE test_tbl1(col1 VARCHAR2(5), col2 VARCHAR2(5), col3 VARCHAR2(5));
    ```

5. Use the `LOAD DATA` statement with URL external table syntax to import data into the table `test_tbl1`, and specify error diagnostics.

    ```sql
    LOAD DATA FROM FILES(
        LOCATION = '/home/admin/test_csv', 
        FORMAT = (
            TYPE = 'csv',
            FIELD_DELIMITER = ','),
        PATTERN = 'column_conv.csv')
        INTO TABLE test_tbl1
        LOG ERRORS REJECT LIMIT UNLIMITED;
    ```

    The result returned is as follows:

    ```shell
    Query OK, 1 row affected, 2 warnings
    Records: 1  Deleted: 0  Skipped: 0  Warnings: 2
    ```

6. View the data in `test_tbl1`.

    ```sql
    SELECT * FROM test_tbl1;
    ```

    The result returned is as follows:

    ```shell
    +------+-------+-------+
    | COL1 | COL2  | COL3  |
    +------+-------+-------+
    | 1    | short | short |
    +------+-------+-------+
    1 row in set
    ```

## References

* For more information about the `LOAD DATA` statement, see [Import data by using the LOAD DATA statement](../../../../../../500.data-migration/700.migrate-data-from-csv-file-to-oceanbase-database/200.use-the-load-command-to-load-the-csv-data-file-to-the-oceanbase-database.md).
* For more information about direct load by using the `LOAD DATA` statement, see [Import data by using the LOAD DATA statement](../../../../../../620.obap/300.obap-import-data/20.bypass-import/200.full-bypass-import.md).
