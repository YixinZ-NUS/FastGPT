| Description   |                 |
|---------------|-----------------|
| keywords      |                 |
| dir-name      |                 |
| dir-name-en   |                 |
| tenant-type   | MySQL Mode      |
|machine-translation||

# TOKENIZE

## Description

The function takes a text string and a tokenizer as input, and returns the tokenized result in JSON format.

## Syntax

```sql
TOKENIZE('text', ['parser'], ['behavior_ctrl'])
```

## Parameters

|    **Parameter**   | **Description** |
|---------------|----------|
| text          | The text to be tokenized. Valid data types are `TEXT`, `CHAR`, and `VARCHAR`.|
| parser        | The tokenizer name. Valid values include `BENG` (basic English), `NGRAM` (Chinese), `SPACE` (space), and `IK` (Chinese).<main id="notice" type='explain'><h4>Note</h4><p>For OceanBase Database V4.3.5, the <code>IK</code> tokenizer was supported starting from V4.3.5 BP1. </p></main></li></ul> |
| behavior_ctrl | JSON-formatted parameters. Valid values include:<ul><li>`stopwords`: the stopword table. If this parameter is not specified, the global default stopword table is used. If this parameter is specified but empty, no stopword table is used.</li><li>`case`: the case of the tokenized result. If this parameter is not specified, the system default behavior is used. Otherwise, the tokenized result is in upper or lower case.</li><li>`output`: the format of the output. Valid values include:<ul><li>`default`: the `json array` containing only the tokens, such as `["hello", "world", "english"]`. </li><li>`all`: the `json object` containing the `doc_len` and token frequency, such as `{"doc_len":3, "tokens":["i":1, "love":1, "china":1]}`. </li><li>`additional-args`: the parameters of a specific tokenizer, such as `["token_size:2"]} for the `ngram` tokenizer. </li></ul>|

## Examples

The following example tokenizes the string `I Love China` by using the `beng` tokenizer and sets the output options in JSON format.

```sql
SELECT TOKENIZE('I Love China','beng', '[{"output": "all"}]');
```

The return result is as follows:

```shell
+--------------------------------------------------------+
| TOKENIZE('I Love China','beng', '[{"output": "all"}]') |
+--------------------------------------------------------+
| {"tokens": [{"love": 1}, {"china": 1}], "doc_len": 2}  |
+--------------------------------------------------------+
1 row in set
```