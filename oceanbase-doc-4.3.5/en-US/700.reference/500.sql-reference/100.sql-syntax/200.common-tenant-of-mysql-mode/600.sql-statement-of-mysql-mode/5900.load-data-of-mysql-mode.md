| Description   |                 |
|---------------|-----------------|
| keywords      |                 |
| dir-name      |                 |
| dir-name-en   |                 |
| tenant-type   | MySQL Mode      |
|machine-translation||

# LOAD DATA

## Purpose

This statement is used to import data from an external source.

* **The `LOAD DATA` statement of OceanBase Database supports the following input files:**

  * **Server-side (OBServer node) files**: Files located on the OBServer nodes of OceanBase Database. You can use the `LOAD DATA INFILE` statement or the `LOAD DATA FROM URL` statement to load data from server-side files into database tables.

  * **Client-side (local) files**: Files located in the local file system of the client. You can use the `LOAD DATA LOCAL INFILE` statement or the `LOAD DATA FROM URL` statement to load data from client-side local files into database tables.

    <main id="notice" type='explain'>
      <h4>Note</h4>
      <p>When OceanBase Database executes the <code>LOAD DATA LOCAL INFILE</code> statement, the system automatically adds the <code>IGNORE</code> option.</p>
    </main>

  * **OSS files**: Files stored in an OSS file system. You can use the `LOAD DATA REMOTE_OSS INFILE` statement to load data from OSS files to a database table.

* **`LOAD DATA` currently supports importing text files in CSV format. The entire import process can be divided into the following steps:**

  1. **File parsing**: OceanBase Database reads the data from the file specified by the user and determines whether to parse the input file data in parallel or serially based on the specified degree of parallelism.

  2. **Data distribution**: Since OceanBase Database is a distributed database, data for different partitions may be distributed across different OBServer nodes. `LOAD DATA` calculates the parsed data and determines which OBServer node the data should be sent to.

  3. **Data insertion**: Once the target OBServer node receives the data, it performs the `INSERT` operation locally to insert the data into the corresponding partition.

## Considerations

* Tables with triggers are not allowed to use the <code>LOAD DATA</code> statement.
* To import data from external files, you need to have the <code>FILE</code> privilege and the following configurations:

  * When loading server-side files, you need to set the system variable <a href="../../../../800.configuration-items-and-system-variables/200.system-variable/300.global-system-variable/12000.secure_file_priv-global.md">secure_file_priv</a> in advance to configure the accessible path for importing or exporting files.
  * When loading client-side local files, you need to add the <code>--local-infile[=1]</code> option when starting the MySQL/[OBClient](https://www.oceanbase.com/softwarecenter-cloud) client to enable the functionality of loading data from the local file system.

* When using specified partition bypass import, note that the target table cannot be a replicated table, and it must not contain auto-increment columns, identity columns, or a Global Index.

## Syntax

```sql
-- Load data from a regular file.
LOAD DATA
    [/*+ PARALLEL(N) [load_batch_size(M)] [APPEND | direct(bool, int, [load_mode])] | NO_DIRECT */]
    [REMOTE_OSS | LOCAL] INFILE 'file_name'
    [REPLACE | IGNORE]
    INTO TABLE table_name [PARTITION(PARTITION_OPTION)] 
    [COMPRESSION [=] {AUTO|NONE|GZIP|DEFLATE|ZSTD}]
    [{FIELDS | COLUMNS}
        [TERMINATED BY 'string']
        [[OPTIONALLY] ENCLOSED BY 'char']
        [ESCAPED BY 'char']
    ]
    [LINES
        [STARTING BY 'string']
        [TERMINATED BY 'string']
    ]
    [IGNORE number {LINES | ROWS}]
    [(column_name_var
        [, column_name_var] ...)]
    
load_mode:
    'full' 
    | 'inc_replace'

PARTITION_OPTION:
    partition_option_list
    | subpartition_option_list

-- Load data from a URL.
LOAD DATA
    [/*+ PARALLEL(N) [load_batch_size(M)] [APPEND | direct(bool, int, [load_mode])] | NO_DIRECT */]
    [REPLACE | IGNORE]
    FROM { url_table_function_expr |
         ( SELECT expression_list FROM url_table_function_expr ) }
    INTO TABLE table_name
    [PARTITION(PARTITION_OPTION)] 
    [(column_name_var [, column_name_var] ...)]
    [LOG ERRORS [REJECT LIMIT {integer | UNLIMITED}]]

load_mode:
    'full' 
    | 'inc_replace'

url_table_function_expr:

  FILES (
    LOCATION = '<string>',
    {
      FORMAT = (
        TYPE = 'CSV',
        LINE_DELIMITER = '<string>' | <expr>,
        FIELD_DELIMITER = '<string>' | <expr>,
        PARSE_HEADER = { TRUE | FALSE },
        ESCAPE = '<character>' | <expr>,
        FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | <expr>,
        ENCODING = 'charset',
        NULL_IF = ('<string>' | <expr>, '<string>' | <expr> ...),
        SKIP_HEADER = <int>,
        SKIP_BLANK_LINES = { TRUE | FALSE },
        TRIM_SPACE = { TRUE | FALSE },
        EMPTY_FIELD_AS_NULL = { TRUE | FALSE }
      )
      | FORMAT = ( TYPE = 'PARQUET' | 'ORC' )
    },
    [PATTERN = '<regex_pattern>']
  )
  | SOURCE (
      TYPE = 'ODPS',
      ACCESSID = '<string>',
      ACCESSKEY = '<string>',
      ENDPOINT = '<string>',
      TUNNEL_ENDPOINT = '<string>',
      PROJECT_NAME = '<string>',
      SCHEMA_NAME = '<string>',
      TABLE_NAME = '<string>',
      QUOTA_NAME = '<string>',
      COMPRESSION_CODE = '<string>'
    )

PARTITION_OPTION:
    partition_option_list
    | subpartition_option_list

```

## Parameters

| Parameter | Description |
|-----------|-------------|
| parallel(N)                      | The degree of parallelism for loading data, with a default value of 4.   |
| load_batch_size(M)               | Specifies the batch size for each insertion. The default value of `M` is `100`. The recommended range is [100, 1000].    |
| APPEND \| direct() \|NO_DIRECT   | Uses hints to enable direct load.<main id="notice" type='notice'><h4>Notice</h4><p>During a direct load task, we recommend that you do not upgrade OceanBase Database, because this may cause the direct load task to fail. </p></main> <ul><li>The `APPEND` hint is equivalent to using `direct(true, 0)` by default. It also allows you to collect statistics online (`GATHER_OPTIMIZER_STATISTICS` hint). </li><li>The `direct()` parameter is described as follows:<ul><li><code>bool</code>: specifies whether to sort the data to be written. The value <code>true</code> specifies to sort the data, and the value <code>false</code> specifies not to sort the data. </li><li><code>int</code>: specifies the maximum number of tolerated error rows. </li><li><code>load_mode</code>: an optional parameter that specifies the direct load mode. The value must be enclosed in single quotation marks (' ') and can be one of the following:<ul><li><code>full</code>: specifies a full direct load. This is the default value. <main id="notice" type='notice'><h4>Notice</h4><p>For a full direct load with a unique index, if a duplicate unique key is found, the <code>REPLACE</code> or <code>IGNORE</code> keyword cannot be used, and the error tolerance row cannot be set. </p></main></li><li><code>inc</code>: specifies an incremental direct load that supports the <code>INSERT</code> and <code>IGNORE</code> semantics. </li><li><code>inc_replace</code>: specifies an incremental direct load that does not check for duplicate primary keys. This is equivalent to an incremental direct load with the <code>REPLACE</code> semantics. <main id="notice" type='notice'><h4>Notice</h4><p>If the value of <code>load_mode</code> is <code>inc_replace</code>, the <code>LOAD DATA</code> statement must not contain the <code>REPLACE</code> or <code>IGNORE</code> keyword. </p></main></li></ul></li></ul></li><li><code>NO_DIRECT</code>: a hint that controls the execution of a single SQL statement to bypass direct load. If this hint is specified in an SQL statement, other hints for direct load in the statement are ignored, and the SQL statement is executed as a normal load. </li></ul> For more information about using the `LOAD DATA` statement for direct load, see [Import data or files by using the LOAD DATA statement](../../../../../620.obap/300.obap-import-data/20.bypass-import/200.full-bypass-import.md). |
| REMOTE_OSS \| LOCAL | Optional. Specifies whether to read data from an object storage service or the local file system of the client.<ul><li> <code>REMOTE_OSS</code> specifies whether to read data from an object storage service. Currently, Alibaba Cloud OSS, Amazon S3, and object storage services that are compatible with S3 such as OBS, GCS, and COS are supported. </li><li> <code>LOCAL</code> specifies whether to read data from the local file system of the client. </li><li>If you do not specify <code>REMOTE_OSS</code> or <code>LOCAL</code>, data in the file system of the OBServer node is imported. </li></ul> |
| file_name           | The path and name of the input file. The format of the <code>file_name</code> parameter corresponds to the type of <code>REMOTE_OSS \| LOCAL</code>:<ul><li>When you specify <code>REMOTE_OSS</code>, the value of <code>file_name</code> indicates the path and name of the file in the object storage service. The following examples show the paths and names of files in different object storage services:<ul><li><b>Alibaba Cloud OSS</b>: <code>LOAD DATA /\*+ [APPEND \| DIRECT(need_sort,max_error)] parallel(N) \*/ REMOTE_OSS INFILE 'oss://\$PATH/\$FILENAME/?host=\$HOST&access_id=\$ACCESS_ID&access_key=\$ACCESS_KEY' INTO TABLE table_name ...;</code></li><li><b>Amazon S3</b>: <code>LOAD DATA /\*+ [APPEND \| DIRECT(need_sort,max_error)] parallel(N) \*/ REMOTE_OSS INFILE 's3://\$PATH/\$FILENAME/?host=\$HOST&access_id=\$ACCESS_ID&access_key=\$ACCESS_KEY&s3_region=S3_REGION' INTO TABLE table_name ...;</code></li><li><b>Object storage services that are compatible with S3</b>: <code>LOAD DATA /\*+ [APPEND \| DIRECT(need_sort,max_error)] parallel(N) \*/ REMOTE_OSS INFILE 's3://\$PATH/\$FILENAME/?host=\$HOST&access_id=\$ACCESS_ID&access_key=\$ACCESS_KEY' INTO TABLE table_name ...;</code></li></ul></li><li>When you specify <code>LOCAL</code>, the value of <code>file_name</code> indicates the path and name of the file in the local file system of the client. The syntax is <code>LOAD DATA /*+ [APPEND \| DIRECT(need_sort,max_error)] parallel(N) */ INFILE '/\$PATH/\$FILENAME' INTO TABLE table_name ...;</code>. </li><li>If you do not specify <code>REMOTE_OSS</code> or <code>LOCAL</code>, the syntax is the same as that when you specify <code>LOCAL</code>. </li></ul>The following table describes the parameters in the syntax. <ul><li><code>\$PATH</code>: the path of the file in the bucket, which indicates the directory where the file is located. </li><li><code>\$FILENAME</code>: the name of the file, which indicates the specific file to be accessed. </li><li><code>\$HOST</code>: the host name or domain name of the server where the OSS service is located or the server where the CDN is accelerated. It indicates the address where the OSS service is located. </li><li><code>\$ACCESS_ID</code>: the Access Key ID required for accessing the OSS service. It is used for authentication. </li><li><code>\$ACCESSKEY</code>: the Access Key Secret required for accessing the OSS service. It is used for authentication. </li><li><code>S3_REGION</code>: the region where the Amazon S3 bucket is located. It is a required parameter when the object storage service is Amazon S3. </li></ul><main id="notice" type='explain'><h4>Note</h4> <p>When you import a file from OSS, make sure of the following:<ul><li>You have the permissions to access the OSS bucket and file. You must have sufficient permissions to read the specified bucket and file. This usually requires that you set the permissions in the OSS console or by using OSS APIs and configure the access keys (Access Key ID and Access Key Secret) as credentials with appropriate permissions. </li><li>The database server can connect to the specified <code>$HOST</code> over the network to access the OSS service. If the <code>$HOST</code> is a domain name accelerated with CDN, the CDN is correctly configured and the network connection is normal. </li></ul></p> </main>  |
| REPLACE \| IGNORE                | If a unique key conflict occurs, `REPLACE` means to overwrite the conflicting row, while `IGNORE` means to skip the conflicting row. `LOAD DATA` uses the primary key of the table to determine if the data is duplicate. If the table has no primary key, there is no difference between the `REPLACE` and `IGNORE` options. By default, when duplicate data is encountered, `LOAD DATA` logs the erroneous data to a log file.  <main id="notice" type='notice'><h4>Note</h4><p><ul><li>When executing the <code>LOAD DATA LOCAL INFILE</code> command in MySQL mode, the system automatically adds the <code>IGNORE</code> option. This behavior provides better compatibility with MySQL databases.</li><li>When using the <code>REPLACE</code> or <code>IGNORE</code> clause, if the degree of parallelism is greater than <code>1</code>, the final inserted record for conflicting rows might differ from the result of serial execution. To strictly ensure the insertion result of conflicting records, do not specify the parallelism of the statement (or set the parallelism to <code>1</code>).</li></ul></p></main> |
| url_table_function_expr | Optional. Data is read from the file system or data source by using the FILES or SOURCE keyword. <ul><li>FILES describes the location and format of the data to be imported. For more information about the use of FILES, see [FILES](#FILES). </li> <li>SOURCE defines the extraction of data from the data source in ODPS format. For more information about the use of SOURCE, see [SOURCE](#SOURCE). </li></ul> |
| table_name                       | The name of the table to which the imported data belongs. <ul><li>You can specify partitioned or non-partitioned tables. </li><li>You can specify any number of columns in the table. </li></ul>  |
| PARTITION_OPTION                   | The name of the partition for direct load. You can specify multiple partitions by separating them with commas (,).<ul><li>partition_option_list: a list of partition names for direct load. </li> <li>subpartition_option_list: a list of subpartition names for direct load. </li></ul><main id="notice" type='explain'><h4>Note</h4><p>You can import data only by using direct load to a specified partition. Normal LOAD DATA does not support specifying partitions. If you do not specify direct load hints or parameters, specifying partitions in the LOAD DATA statement is invalid. </p></main>|
| COMPRESSION                      | The compression format of the file. The valid values are described as follows, where<ul><li><code>AUTO</code>: automatically detects the compression algorithm based on the file name suffix. <br> When you use the <code>AUTO</code> parameter, different suffixes correspond to specific compression formats, as described in the following table. <ul><li><code>.gz</code>: a GZIP-compressed file. </li><li><code>.deflate</code>: a DEFLATE-compressed file. </li><li><code>.zst/.zstd</code>: a ZSTD-compressed file. </li></ul></li><li><code>NONE</code>: indicates that the file is not compressed. </li><li><code>GZIP</code>: a GZIP-compressed file. </li><li><code>DEFLATE</code>: a DEFLATE-compressed file without metadata. </li><li><code>ZSTD</code>: a ZSTD-compressed file. </li></ul> You can explicitly specify the compression format of a file or let the system detect the compression format based on the file name suffix. |
| FIELDS \| COLUMNS                | The format of fields. <ul><li> `ENCLOSED BY`: the delimiter that encloses the exported values.  </li> <li> `TERMINATED BY`: the delimiter that terminates the exported columns. </li> <li>`ESCAPED BY`: the character that is ignored in exported values. </li></ul>    |
| LINES STARTING BY                | The delimiter that indicates the start of a line.   |
| LINES TERMINATED BY              | The delimiter that indicates the end of a line.  |
| IGNORE number { LINES \| ROWS } | The number of the first few lines to be ignored. `LINES` indicates the first few lines of the file, and `ROWS` indicates the first few rows separated by the field delimiter. By default, the system matches each input file column with a table column. If the input file does not contain all the columns in the table, the system fills in the missing columns based on the following rules:  <ul><li> Character type: an empty string.   </li> <li> Numeric type: 0.    </li> <li> Date type: `0000-00-00`.</li></ul><main id="notice" type='explain'><h4>Note</h4><p>The behavior is the same for multi-file import as it is for single-file import. </p></main>    |
| column_name_var                  | The name of the column to which the imported data belongs.  |
| LOG ERRORS [REJECT LIMIT {integer \| UNLIMITED}] | Optional. Enables error diagnostics during the process of importing URL external tables. For detailed information, refer to [log_errors](#log_errors) below. <main id="notice" type='explain'><h4>Note</h4><p>For OceanBase Database V4.3.5, the syntax for the `LOAD DATA` statement to import URL external tables supports specifying error diagnostics starting from  V4.3.5 BP2.</p></main> |

### FILES

The `FILES` keyword consists of the clauses `LOCATION`, `FORMAT`, and `PATTERN`.

* The `LOCATION` clause is used to specify the path where the external table files are stored. Typically, the data files for the external table are stored in a single directory. The directory can include subdirectories, and when the table is created, the external table will automatically collect all files in that directory.

  * The local `LOCATION` format is `LOCATION = '[file://] local_file_path'`, where `local_file_path` can be either a relative path or an absolute path. If a relative path is specified, the current directory must be the installation directory of the OceanBase database. The `secure_file_priv` parameter is used to configure the file paths that OBServer nodes have permission to access. The `local_file_path` must be a subpath of the `secure_file_priv` path.

  * The remote `LOCATION` format is as follows:  
    * `LOCATION = '{oss|cos|S3}://$ACCESS_ID:$ACCESS_KEY@$HOST:s3_region/remote_file_path'`, where `$ACCESS_ID`, `$ACCESS_KEY`, and `$HOST` are the access credentials required to connect to OSS, COS, or S3. The `s3_region` specifies the region information for S3. These sensitive access credentials are stored in the database's system tables in an encrypted format.  
    * `LOCATION = 'hdfs://${hdfs_namenode_address}:${port}/PATH.localhost'`, where `port` refers to the port number of HDFS, and `PATH` specifies the directory path in HDFS.  
      * With Kerberos authentication: `LOCATION = 'hdfs://localhost:port/user?principal=xxx&keytab=xxx&krb5conf=xxx&configs=xxx'`.
        Where:
        * `principal`: the authenticated user.  
        * `keytab`: the key file for user authentication.  
        * `krb5conf`: the Kerberos environment description file used by the user.  
        * `configs`: additional HDFS configuration options. By default, this is empty. However, in a Kerberos environment, this configuration typically needs to be set. For example, `dfs.data.transfer.protection=authentication,privacy`, which specifies the data transfer protection level as `authentication` and `privacy`.

    <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>When using an object storage path, the parameters of the object storage path are separated by the <code>&</code> symbol. Please ensure that the parameter values you enter only contain uppercase and lowercase English letters, numbers, <code>\/-_$+=</code>, and wildcards. Entering characters outside of these may result in a failure to set the parameters.</p>
    </main>

* The `FORMAT` clause specifies attributes related to the file format. Valid values: CSV, PARQUET, and ORC.

  * When **TYPE = 'CSV'**, the following fields are included:

  * `LINE_DELIMITER`: the line delimiter in the CSV file. The default value is `LINE_DELIMITER='\n'`.  
  * `FIELD_DELIMITER`: optional. Specifies the column delimiter in the CSV file. The default value is `FIELD_DELIMITER='\t'`.  
  * `PARSE_HEADER`: optional. Specifies whether the first line of the CSV file is the column name for each column. The default value is `FALSE`, meaning the first line of the CSV file is not treated as column names.  
  * `ESCAPE`: the escape character in the CSV file. It must be a single byte. The default value is `ESCAPE='\'`.  
  * `FIELD_OPTIONALLY_ENCLOSED_BY`: optional. Specifies the symbol used to enclose field values in the CSV file. The default value is empty. Using this option means that only certain types of fields (such as CHAR, VARCHAR, TEXT, and JSON) will be enclosed.  
  * `ENCODING`: the character set encoding format of the file. If not specified, the default value is `UTF8MB4`.  
  * `NULL_IF`: the string that should be treated as `NULL`. The default value is empty.  
  * `SKIP_HEADER`: skips the file header and specifies the number of lines to skip.  
  * `SKIP_BLANK_LINES`: specifies whether to skip blank lines. The default value is `FALSE`, meaning blank lines are not skipped.  
  * `TRIM_SPACE`: specifies whether to remove leading and trailing spaces from fields in the file. The default value is `FALSE`, meaning leading and trailing spaces are not removed.  
  * `EMPTY_FIELD_AS_NULL`: specifies whether to treat empty strings as `NULL`. The default value is `FALSE`, meaning empty strings are not treated as `NULL`.  

* When **TYPE = 'PARQUET/ORC'**, there are no additional fields.

* The `PATTERN` clause specifies a regular pattern string for filtering files in the directory specified by the `LOCATION` clause. For each file in the directory specified by the `LOCATION` clause, if the file path matches the pattern string, the external table accesses the file. Otherwise, the external table skips the file. If this parameter is not specified, the external table accesses all files in the directory specified by the `LOCATION` clause by default.

### SOURCE

The `SOURCE` clause takes no other clauses and contains the following fields when **TYPE = 'ODPS'**:

* `ACCESSID`: specifies the ID of the ODPS user.
* `ACCESSKEY`: specifies the password of the ODPS user.
* `ENDPOINT`: specifies the connection address of the ODPS service.
* `TUNNEL_ENDPOINT`: specifies the connection address of the Tunnel data transmission service.
* `PROJECT_NAME`: specifies the project in which the table to be queried is located.
* `SCHEMA_NAME`: optional. Specifies the schema in which the table to be queried is located.
* `TABLE_NAME`: specifies the name of the table to be queried.
* `QUOTA_NAME`: optional. Specifies whether to use the specified quota.
* `COMPRESSION_CODE`: optional. Specifies the compression format of the data source. Valid values: `ZLIB`, `ZSTD`, `LZ4`, and `ODPS_LZ4`. The default value is an empty string, which specifies not to enable compression.

### log_errors

* `LOG ERRORS`: Enables error diagnostics during the import process, allowing failed rows to be recorded (currently recorded in the `warning buffer`, viewable via `show warnings`, with a maximum of 64 rows), rather than terminating the entire operation due to the first error. It can be used with the `REJECT LIMIT` clause to control the number of allowable error rows.

* `REJECT LIMIT`: Optional, used to set the maximum allowable number of error rows:

  * Default value is 0: No error rows are allowed, and the operation fails upon encountering the first error.
  * `integer`: The maximum number of error rows allowed per machine. For example, 10 means up to 10 error rows are allowed on a single machine.
  * `UNLIMITED`: Allows an unlimited number of error rows.

<main id="notice" type='notice'>
  <h4>Notice</h4>
  <p><ul><li>If the <code>LOG ERRORS</code> clause is not specified, the behavior is normal import behavior, meaning an error is reported upon encountering the first error.</li><li>If the <code>LOG ERRORS</code> clause is specified but the <code>REJECT LIMIT</code> clause is not specified, it is equivalent to specifying a diagnostic with a <code>LIMIT</code> of 0, causing the operation to fail upon encountering the first error, but the first encountered error will be recorded, and the error code will be related to diagnostics, specifically "reject limit reached".</li></ul></p>
</main>

### Wildcard rules for direct load of multiple files

To facilitate the import of multiple files, the wildcard feature is introduced for server-side and OSS file imports, but not for client-side file imports.

* **Use of wildcards on the server**

  * **Matching rules:**

    * Matching filenames: `load data /*+ parallel(20) direct(true, 0) */ infile '/xxx/test.*.csv' replace into table t1 fields terminated by '|';`

    * Matching directories: `load data /*+ parallel(20) direct(true, 0) */ infile '/aaa*bb/test.1.csv' replace into table t1 fields terminated by '|';`

    * Matching both directories and filenames: `load data /*+ parallel(20) direct(true, 0) */ infile '/aaa*bb/test.*.csv' replace into table t1 fields terminated by '|';`

  * **Considerations:**

    * At least one matching file must exist. Otherwise, an error code 4027 is returned.

    * For `load data /*+ parallel(20) direct(true, 0) */ infile '/xxx/test.1*.csv,/xxx/test.6*.csv' replace into table t1 fields terminated by '|';`, `/xxx/test.1*.csv,/xxx/test.6*.csv` will be considered as a whole for matching. If no file matches, an error code 4027 will be returned.

    * Only wildcards supported by the POSIX GLOB function can be used. For example, `test.6*(6|0).csv` and `test.6*({0.csv,6.csv}|.csv)` are allowed. Although these wildcards can be used, they will not match any result, and an error code 4027 will be returned when you use them.

* **Use of wildcards in object storage service (OSS)**

  * **Matching rules:**

    Matching filenames: `load data /*+ parallel(20) direct(true, 0) */ remote_oss infile 'oss://xxx/test.*.csv?host=xxx&access_id=xxx&access_key=xxx' replace into table t1 fields terminated by '|';`

  * **Considerations:**

    * Directory matching is not supported. For example, `load data /*+ parallel(20) direct(true, 0) */ remote_oss infile 'oss://aa*bb/test.*.csv?host=xxx&access_id=xxx&access_key=xxx' replace into table t1 fields terminated by '|';` will return `OB_NOT_SUPPORTED`.

    * Only `*` and `?` are supported as wildcard characters for filenames. Other wildcard characters can be entered but will not match any result.

## Examples

<main id="notice" type='explain'>
  <h4>Note</h4>
  <p>In a <code>LOAD DATA</code> statement, <code>\N</code> represents <code>NULL</code>.</p>
</main>

### Import data from a server

**Example 1: Import data from a server.**

1. Set the global secure path.

   ```shell
   obclient> SET GLOBAL secure_file_priv = "/"
   Query OK, 0 rows affected
   obclient> \q
   Bye
   ```

   <main id="notice" type='explain'>
     <h4>Note</h4>
     <p>Since <code>secure_file_priv</code> is a <code>GLOBAL</code> variable, you need to execute <code>\q</code> to make the setting take effect.</p>
   </main>

2. Reconnect to the database and import data from an external file.

   ```shell
   obclient> LOAD DATA INFILE 'test.sql' INTO TABLE t1;
   Query OK, 0 rows affected
   ```

**Example 2: Use the `APPEND` hint to enable direct load.**

```shell
LOAD DATA /*+ PARALLEL(4) APPEND */
   INFILE '/home/admin/a.csv'
   INTO TABLE t;
```

**Example 3: Import a CSV file.**

  * Import all columns from the `test1.csv` file.

    ```shell
    load data  /*+ direct(true,0) parallel(2)*/
    from files(
      location = "data/csv",
      format = (
        type = 'csv',
        field_delimiter = ',',
        parse_header = true,
        skip_blank_lines = true
      ),
      pattern = 'test1.csv')
    into table t1;
    ```

  * Read the `c1` and `c2` columns from the `test1.csv` file in the `data/csv` directory and import them into the `col1` and `col2` columns of the `t1` table.

    ```shell
    load data  /*+ direct(true,0) parallel(2)*/
    from (
      select c1, c2 from files(
          location = 'data/csv'
            format = (
            type = 'csv',
            field_delimiter = ',',
            parse_header = true,
            skip_blank_lines = true
          ),
          pattern = 'test1.csv'))
    into table t1 (col1, col2);
    ```

**Example 4: Import a PARQUET file.**

```shell
load data  /*+ direct(true,0) parallel(2)*/
from files(
  location = "data/parquet",
  format = ( type = 'PARQUET'),
  pattern = 'test1.parquet')
into table t1;
```

**Example 5: Import an ORC file.**

```shell
load data  /*+ direct(true,0) parallel(2)*/
from files(
  location = "data/orc",
  format = ( type = 'ORC'),
  pattern = 'test1.orc')
into table t1;
```

**Example 5: Import an ODPS file.**

```shell
load data  /*+ direct(true,0) parallel(2)*/
from source (
  type = 'ODPS',
  accessid = '$ODPS_ACCESSID',
  accesskey = '******',
  endpoint= '$ODPS_ENDPOINT',
  project_name = 'example_project',
  schema_name = '',
  table_name = 'example_table',
  quota_name = '',
  compression_code = '')
into table t1;
```

### Import data from a local file

**Example 1: Import data from a local file to a table in OceanBase Database.**

1. Open the terminal or command prompt window and enter the following command to start the client.

    ```shell
    obclient --local-infile -hxxx.xxx.xxx.xxx -P2881 -uroot@mysql001 -p****** -A -Dtest
    ```

    The return result is as follows:

    ```shell
    Welcome to the OceanBase.  Commands end with ; or \g.
    Your OceanBase connection id is 3221719526
    Server version: OceanBase 4.2.2.0 (r100000022023121309-f536833402c6efe9364d5a4b61830a858ef24d82) (Built Dec 13 2023 09:58:18)

    Copyright (c) 2000, 2018, OceanBase and/or its affiliates. All rights reserved.

    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

    obclient [test]>
    ```

    <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>To use the <code>LOAD DATA LOCAL INFILE</code> feature, you must use OBClient V2.2.4 or later. If you do not have OBClient of the required version, you can connect to the database using a MySQL client.</p>
    </main>

2. In the client, execute the `LOAD DATA LOCAL INFILE` statement to load data from the local file.

    ```shell
    obclient [test]> LOAD DATA LOCAL INFILE '/home/admin/test_data/tbl1.csv' INTO TABLE tbl1 FIELDS TERMINATED BY ',';
    ```

    The return result is as follows:

    ```shell
    Query OK, 3 rows affected
    Records: 3  Deleted: 0  Skipped: 0  Warnings: 0
    ```

**Example 2: Directly import a compressed file by setting COMPRESSION.**

```shell
LOAD DATA LOCAL INFILE '/your/file/lineitem.tbl.gz' 
    INTO TABLE lineitem 
    COMPRESSION GZIP 
    FIELDS TERMINATED BY '|';
```

**Example 3: Specify a partition for direct load.**

* Specify a partition by using the partitioning key.

```shell
load data /*+ direct(true,0) parallel(2) load_batch_size(100) */ 
infile "$FILE_PATH" 
into table t1 partition(p0, p1) 
fields terminated by '|' enclosed by '' lines starting by '' terminated by '\n';
```

* Specify a subpartition by using the subpartitioning key.

```shell
load data /*+ direct(true,0) parallel(2) load_batch_size(100) */ 
infile "$FILE_PATH" 
into table t1 partition(p0sp0, p1sp1) 
fields terminated by '|' enclosed by '' lines starting by '' terminated by '\n';
```

### Import data from an OSS file

**Example 1: Use the `direct(bool, int)` hint to enable direct load, where the file to be loaded is stored in OSS.**

```shell
load data /*+ parallel(1) direct(false,0)*/ remote_oss infile 'oss://antsys-oceanbasebackup/backup_rd/xiaotao.ht/lineitem2.tbl?host=***.oss-cdn.***&access_id=***&access_key=***' into table lineitem fields terminated by '|' enclosed by '' lines starting by '' terminated by '\n';
```

### Import data from a server-side file by using URL external tables

<main id="notice" type='notice'>
<h4>Notice</h4>
<p>The commands involving IP addresses in the example are desensitized. Replace the IP addresses with the actual ones when you verify the commands. </p>
</main>

The following example describes how to create URL external tables by using the location of the external file on a server and in MySQL-compatible mode of OceanBase Database. The steps are as follows:

1. Create an external file.

    Run the following command to create a file named `column_conv.csv` in the `/home/admin/test_csv` directory on the server where the OBServer node to log in to is located.

    ```shell
    [admin@xxx /home/admin/test_csv]# vi column_conv.csv
    ```

    The content of the file is as follows:

    ```shell
    1,short,short
    2,long_text,long_text
    3,long_text,long_text
    ```

2. Set the path of the imported file.

    <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>For security reasons, when you set the system variable <code>secure_file_priv</code>, you can connect to the database only through a local socket to execute the SQL statement that changes the global variable. For more information, see <a href="../../../../800.configuration-items-and-system-variables/200.system-variable/300.global-system-variable/12000.secure_file_priv-global.md">secure_file_priv</a>. </p>
    </main>

    1. Run the following command to log in to the server where the OBServer node is located.

        ```shell
        ssh admin@10.10.10.1
        ```

    2. Run the following command to connect to the `mysql001` tenant through a local Unix socket.

        ```shell
        obclient -S /home/admin/oceanbase/run/sql.sock -uroot@mysql001 -p******
        ```

    3. Run the following SQL command to set the import path to `/home/admin/test_csv`.

        ```sql
        SET GLOBAL secure_file_priv = "/home/admin/test_csv";
        ```

3. Reconnect to the `mysql001` tenant.

    Here is an example:

    ```shell
    obclient -h10.10.10.1 -P2881 -uroot@mysql001 -p****** -A -Ddb_test
    ```

4. Create a table named `test_tbl1`.

    ```sql
    CREATE TABLE test_tbl1(col1 VARCHAR(5), col2 VARCHAR(5), col3 VARCHAR(5));
    ```

5. Use the `LOAD DATA` statement to import data from the URL external table to the `test_tbl1` table and specify error diagnostics.

    ```sql
    LOAD DATA FROM FILES(
        LOCATION = '/home/admin/test_csv', 
        FORMAT = (
            TYPE = 'csv',
            FIELD_DELIMITER = ','),
        PATTERN = 'column_conv.csv')
        INTO TABLE test_tbl1
        LOG ERRORS REJECT LIMIT UNLIMITED;
    ```

    The return result is as follows:

    ```shell
    Query OK, 1 row affected, 2 warnings
    Records: 1  Deleted: 0  Skipped: 0  Warnings: 2
    ```

6. View warnings.

    ```sql
    SHOW warnings;
    ```

    The return result is as follows:

    ```shell
    +---------+------+----------------------------------------------------------------------------------------------------------------------+
    | Level   | Code | Message                                                                                                              |
    +---------+------+----------------------------------------------------------------------------------------------------------------------+
    | Warning | 1406 | fail to scan file column_conv.csv at line 2 for column "db_test"."test_tbl1"."col2", error: Data too long for column |
    | Warning | 1406 | fail to scan file column_conv.csv at line 3 for column "db_test"."test_tbl1"."col2", error: Data too long for column |
    +---------+------+----------------------------------------------------------------------------------------------------------------------+
    2 rows in set
    ```

7. View the data in the `test_tbl1` table.

    ```sql
    SELECT * FROM test_tbl1;
    ```

    The return result is as follows:

    ```shell
    +------+-------+-------+
    | col1 | col2  | col3  |
    +------+-------+-------+
    | 1    | short | short |
    +------+-------+-------+
    1 row in set
    ```

## References

* For more information about how to connect to OceanBase Database, see [Overview of connection methods](../../../../../300.develop/100.application-development-of-mysql-mode/100.connect-to-oceanbase-database-of-mysql-mode/100.connection-methods-overview-of-mysql-mode.md).
* For more information about examples that use the `LOAD DATA` statement, see [Import data by using the LOAD DATA statement](../../../../../500.data-migration/700.migrate-data-from-csv-file-to-oceanbase-database/200.use-the-load-command-to-load-the-csv-data-file-to-the-oceanbase-database.md).
* For more information about examples that use direct load, see [Import data by using the LOAD DATA statement](../../../../../620.obap/300.obap-import-data/20.bypass-import/200.full-bypass-import.md).
