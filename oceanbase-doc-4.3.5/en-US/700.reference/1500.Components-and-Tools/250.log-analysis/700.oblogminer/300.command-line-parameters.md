# Command-line options

You can specify different command-line options to use different features of OceanBase LogMiner (oblogminer). The following table shows the command-line options supported by oblogminer. For more information about how to use these options, see [Examples](400.oblogminer-usage-examples.md).

| Option  | Full option name        | Required? | Type | Default value    | Description |
|-------|------------------|---------|---------|----------|-----|
| -m    | --mode           | No      | String   | analysis | The mode. The default value is analysis. Currently, only the analysis mode is supported. |
| -c    | --cluster-addr   | Yes      | String   | NULL     | The cluster address in the `ip:port` format. If the cluster has multiple addresses, separate them with vertical bars (`\|`). For example, `ip1:mysql_port1|ip2:mysql_port2|ip3:mysql_port3`. To obtain the cluster address, run the following command to query the `DBA_OB_ACCESS_POINT` view: `SELECT CONCAT(SVR_IP, ':' , SQL_PORT) FROM DBA_OB_ACCESS_POINT;`. |
| -u    | --user-name           | Yes      | String   | NULL     | The username of the tenant in the `user@tenant` format. |
| -p    | --password       | Yes      | String   | NULL     | The password of the tenant. This options is required and cannot be empty. |
| -a    | --archive-dest   | Yes      | String   | NULL     | The path of the archived logs. This option cannot be used together with the `-c`, `-u`, and `-p` options. If you specify the `-c`, `-u`, and `-p` options, online logs are read. If you specify the `-a` option, archived logs are read. |
| -l    | --table-list     | No      | String   | \*.\*.\*    | Specifies the names of the databases and tables to analyze. This option has the same format as `tb_white_list` in Oceanbase Change Data Capture (obcdc): `tenant name.database name.table name`. Here is an example: `tenant.db1.tb1|tenant.db2.*`. The tenant name specified in this option must be the same as that specified in the `-u` or `--user-name` option or the name of the tenant to which the archived logs belong. |
| -C    | --column-cond    | No      | String   | NULL     | Specifies column-level filter conditions in the JSON format. |
| -s    | --start-time     | Yes      | String   | NULL     | The start time. oblogminer analyzes only logs generated after the start time. Specify the time in the datatime format or in the 16-digit timestamp format. For example, `2024-01-09 16:23:00` or `1706164790844000`. The start time must meet the requirements of obcdc. If the start time is too early, oblogminer may fail to start. For more information, see [FAQ](../../300.data-integrate/400.cdc/1000.faq-of-obcdc.md). |
| -e    | --end-time       | No      | String   | the current time | The end time. oblogminer analyzes only logs generated before the end time. Specify the time in the datatime format or in the 16-digit timestamp format. For example, `2024-01-09 16:23:00` or `1706164790844000`. |
| -O    | --operations      | No      | String   | insert\|delete\|update | The types of DML operations to display. You can specify any combination of `insert`, `delete`, and `update` by concatenating them with vertical bars (`\|`). By default, all types of DML operations are displayed. |
| -o    | --output         | Yes      | String   | NULL     | The output directory of the data file. The output directory can be a local directory in the format of `file:///output/` or an Alibaba Cloud Object Storage Service (OSS) directory in the format of `oss://$PATH/$FILENAME/?host=$HOST&access_id=$ACCESS_ID&access_key=$ACCESSKEY`. For a local directory, an absolute path is required. |
| -L    | --log_level      | No      | String   | ALL.\*:INFO;PALF.\*:WARN;SHARE.SCHEMA:WARN | The log level of oblogminer. For more information about the log levels supported by OceanBase Database, see [Log levels](../../../../600.manage/800.logging/200.log-level.md). |
| -z    | --timezone       | No      | String   | +8:00    | The time zone. |
| -f    | --record_format  | No      | String   | csv      | The format in which records are stored. The default value is CSV. Valid values: <ul><li>CSV: The file extension is <code>.csv</code>, and all fields are recorded. </li><li>JSON: The file extension is <code>.json</code>, and all fields are recorded. </li><li>REDO_ONLY: The file extension is <code>.sql</code>, and only SQL_REDO results are recorded. </li><li>UNDO_ONLY: The file extension is <code>.sql</code>, and only SQL_UNDO results are recorded. </li></ul> |
| -h    | --help           | No      | N/A       | N/A   | Specifies to display the help information. |
| -v    | --verbose        | No      | N/A       | N/A   | Specifies to display more details and logs in the CLI. |

## Notes

1. The values of string-type options must be enclosed in quotation marks.

2. The value of the `-C` or `--column-cond` option must be in the JSON format. If you do not specify filter conditions, data is not filtered. If you specify filter conditions, only matched table data is displayed. However, the DDL operations on the table are displayed without being filtered. Here is an example:

   ```json
   [
     {
       "database_name":"db1",
       "table_name":"tbl1",
       "column_cond":[
         {
           "col1":"val1",
           "col2":"val2"
         },
         {
           "col3":null
         }
       ]
     }
   ]
   ```

   In the preceding example, `database_name`, `table_name`, and `column_cond` are configured. Their meanings are as follows:

   * `database_name` specifies the name of the database to analyze.

   * `database_name` specifies the name of the table to analyze.

   * `column_cond` specifies the column-level filter conditions for the corresponding `database_name`.`table_name` value.

   When you configure the `-C` or `--column-cond` option, take note of the following considerations:

   * The value of the `-C` or `--column-cond` option must be an array. Enclose the value with square brackets (`[]`). The filter conditions for each table must be a JSON object.

   * All keys and values are of the string type. Enclose them with quotation marks. If the column value is NULL, set it to `null`.

   * The value of the `column_cond` field must be an array. Each object represents a set of conditions. The relationship of the conditions within an object is AND, whereas the relationship of the conditions from different objects is OR. The preceding example displays the modifications to the records of the `db1.tbl1` table where the value of the `col1` column is `val1` and the value of the `col2` column is `val2`, or where the value of the `col3` column is `null`.

   * Escape quotation marks when specifying this option in the CLI. For example, replace `"` with `\"`.

3. The `-o` or `--output` option is used to specify a local directory or an Alibaba Cloud OSS directory as the output directory of the data file. Make sure that the specified directory is empty.

   * For the local file system, the output directory must start with `file://` and be an absolute path. For example, `file:///data/1/logminer_output/`.

   * For Alibaba Cloud OSS, the output directory must start with `oss://` and be in the format of `oss://$PATH/$FILENAME/?host=$HOST&access_id=$ACCESS_ID&access_key=$ACCESSKEY`.

     * `$PATH` specifies the file path in the bucket, which represents the directory where the file is located.

     * `$FILENAME` specifies the name of the file to be accessed.

     * `$HOST` specifies the host name or CDN domain name of Alibaba Cloud OSS, namely, the endpoint of Alibaba Cloud OSS to be accessed.

     * `$ACCESS_ID` specifies the AccessKey ID for accessing Alibaba Cloud OSS.

     * `$ACCESSKEY` specifies the AccessKey secret for accessing Alibaba Cloud OSS.
