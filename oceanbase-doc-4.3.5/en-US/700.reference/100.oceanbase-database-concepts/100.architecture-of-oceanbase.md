|description||
|---|---|
|keywords||
|dir-name||
|dir-name-en||
|tenant-type||

# OceanBase Database architecture

## Overview

OceanBase Database supports the shared-nothing (SN) architecture and the shared-storage (SS) architecture.

## Shared-nothing architecture

In this architecture, all nodes are equal, with each node equipped with its own SQL engine, storage engine, and transaction engine. This setup operates on a cluster of standard PC servers, delivering key advantages such as high scalability, availability, performance, cost-effectiveness, and strong compatibility with mainstream databases.

![OceanBase cluster architecture (SN)](https://obportal.s3.ap-southeast-1.amazonaws.com/doc/img/OceanBase-Database-EN/ob-cluster-arch.png)

OceanBase Database can run on general server hardware and relies on local storage. Servers for distributed deployment are fully equivalent without special hardware requirements. The shared-nothing architecture of OceanBase Database allows the SQL execution engine to perform distributed execution in the database.

OceanBase Database runs an observer process, a single-process program, on each server to support the operation of database instances and stores data and redo logs of transactions in local files.

An OceanBase cluster is deployed across several zones and each zone consists of several servers. The term zone is a logical concept that represents a set of nodes with similar hardware availability in a cluster. It has different meanings in different deployment modes. For example, if a cluster is deployed in one data center, nodes in a zone can belong to the same rack or the same switch. If a cluster is deployed across IDCs, each zone can correspond to one data center.

The data stored by users in a distributed cluster can have multiple replicas, which are used for fault tolerance and to reduce the read pressure. For a single tenant, data in one zone has only one replica, while different zones can store multiple replicas of the same data. A consensus protocol is used to ensure the consistency of data among replicas.

OceanBase Database is designed in a multi-tenant architecture. Each tenant serves as an independent database instance. You can set a specific distributed deployment mode for a tenant. CPU, memory, and I/O resources of one tenant are isolated from those of other tenants.

## Shared-storage architecture

The shared-storage architecture adopts a storage-compute separation architecture. Each tenant stores one copy of data and logs on shared object storage, while caching hot data and logs on local storage of the node.

![OceanBase cluster architecture (SS mode)](https://obportal.s3.ap-southeast-1.amazonaws.com/doc/img/OceanBase-Database-EN/433/shared-storage.png)

For more information about the deployment architecture of OceanBase Database, see [System architecture](../../100.learn-more-about-oceanbase/300.system-architecture.md).

The operation of an OceanBase database is supported by different components in the multi-tenant layer, storage, replication, load balancing, transaction, SQL, and access layers. These components collaborate with each other.

## Multi-tenant layer

OceanBase Database provides the unique multi-tenant feature to simplify the management of multiple business databases deployed on a large scale and reduce resource costs. In an OceanBase cluster, you can create multiple isolated database instances, which are referred to as tenants. From the perspective of applications, each tenant is a separate database. In addition, you can create a tenant in MySQL- or Oracle-compatible mode. After your application is connected to a MySQL-compatible tenant, you can create users and databases in the tenant. The user experience is similar to that with a standalone MySQL database. In the same way, after your application is connected to an Oracle-compatible tenant, you can create schemas and manage roles in the tenant. The user experience is similar to that with a standalone Oracle database. After a new cluster is initialized, a sys tenant is created. The sys tenant is a MySQL-compatible tenant that stores the metadata of the cluster.

To isolate the resources of each tenant, each observer process can have multiple virtual containers known as resource units (UNIT) that belong to different tenants. A resource unit includes CPU and memory resources. The resource units of each tenant across multiple nodes form a resource pool.

## Storage layer

The storage layer provides data storage and access based on tables or table partitions. Each partition stores a tablet of data. A user-defined non-partitioned table can also be considered a tablet.

Data is stored in four layers in a tablet: MemTable, L0 SSTable, L1 SSTable, and Major SSTable. Data related to DML operations, such as insert, update, and delete, is first written into a MemTable. After the size of the MemTable reaches the specified threshold, a mini compaction is performed to write its data to an L0 SSTable on the disk. When the number of L0 SSTables reaches the specified threshold, the L0 SSTables are merged into one L1 SSTable (which also called minor compaction). During the specified daily off-peak hours, the system performs a major compaction to merge all MemTables, L0 SSTables, and L1 SSTables into one major SSTable.

An SSTable consists of macroblocks with a fixed length of 2 MB, and each macroblock consists of microblocks of variable lengths.

During the major compaction of major SSTable microblocks, OceanBase Database uses the dictionary, run-length, constant, or differential encoding method to encode data of each microblock by column. After compression of data in each column, OceanBase Database performs inter-column equivalence encoding or inter-column substring encoding for multiple columns. Encoding helps compress your data to a smaller size. In addition, the extracted in-column characteristic information can speed up subsequent queries.

After data is encoded, OceanBase Database allows you to compress data in a lossless manner by using general compression algorithms. This further increases the data compression ratio.

## Replication layer

At the replication layer, multiple replicas are synchronized by using log streams. Each tablet corresponds to a specific log stream, and each log stream corresponds to multiple tablets. A log stream stores persistent redo logs that are generated by writing data to a corresponding tablet through DML operations. Replicas of a log stream are distributed to different zones. The replication layer maintains a consensus algorithm for the replicas and selects one of the replicas as the leader. Other replicas of the log stream are followers. DML operations and strong-consistency queries on a tablet are performed only on the leader of its corresponding log stream.

Generally, each tenant maintains on each server only one leader of a log stream and possibly multiple followers of other log streams. The total number of log streams for a tenant depends on the values of the `primary_zone` and `locality` parameters.

The leader of the log stream persists the redo log in the local server based on the Paxos protocol and sends the redo log to the followers of the log stream over network. A follower will reply to the leader when it completes the persistence. After the leader confirms that the redo log is persisted on the majority of followers, it considers the redo log persisted. Then, the followers replay the redo log in real time to keep their state consistent with that of the leader.

When a log stream's replica is elected as the leader, it gets a lease. A healthy leader will keep renewing the lease based on the election protocol during the lease period. The leader executes operations only when the lease is valid. The lease ensures the database capabilities to handle exceptions.

The replication layer automatically responds to server failures to guarantee the continuity of database services. When a fault occurs on servers where less than half of the followers are located, the database services are not affected because more than half of the replicas are working normally. If a fault occurs on the server that hosts the leader, the lease will not be renewed. After the lease expires, a new leader is elected from the followers based on the election protocol and granted a new lease. After that, the database services are resumed.

## Load balancing layer

When you create a table or add a new partition, the system selects an appropriate log stream to create a tablet to balance the data and service load. When a tenant property changes, new resources are added, or tablets are not balanced across servers after a long period of use, the load balancing layer rebalances the data and service across servers by splitting and merging log streams, and migrating the log stream replicas.

For example, after you scale out a tenant to add a server, the load balancing layer splits the existing log streams in the tenant, and migrates to the new server an appropriate number of log streams obtained through splitting, to make full use of the added resources. When a tenant scales in, the load balancing layer migrates the log streams on the servers to be dropped to other servers and merges the migrated log streams with the existing log streams on those servers.

After a database runs for a long time, it may experience continuous table creation and deletion and host more data, which can break the load balance even if the number of servers does not change. A typical case is that after you have deleted some tables that are stored on a couple of servers, the number of tablets on these servers is less than that on other servers. As a result, the load balancing layer migrates tablets from the other servers to these servers to balance the load. To ensure the balance, the load balancing layer periodically generates balancing plans to split the log streams on servers with more tablets. The log streams obtained through splitting are used to carry tablets to be migrated, and then migrated to the destination servers and merged with the log streams on the destination servers.

## Transaction layer

The transaction layer ensures the atomicity of committing DML operations on one or more log streams and the multi-version isolation among concurrent transactions.

### Atomicity

The atomicity of committing a transaction is ensured by using the write-ahead log, which records the modifications to transactions on a log stream, even when multiple tablets are involved. When the modification of a transaction involves multiple log streams, each log stream generates and persists its own write-ahead log. The transaction layer uses the two-phase commit protocol to ensure the atomic commit of the transaction.

When a commit is initiated for a transaction that involves multiple log streams, the transaction selects one of the log streams as the coordinator for a two-phase commit. The coordinator communicates with all log streams modified in the transaction to determine whether the write-ahead log is persistent. When all log streams are persistent, the transaction is committed. The coordinator will then drive all log streams to write the commit log (clog) for this transaction, indicating the final state of the transaction. When a follower replays clogs or the database restarts, OceanBase Database recovers the commit state of committed transactions on each log stream based on the clogs.

When a server is down, it is possible that a transaction is in progress and the write-ahead log of the transaction is written before the server is down but no clog is generated. The write-ahead log of each log stream contains a list of all log streams involved in the transaction. OceanBase Database uses this information to identify the coordinator log stream. Then, the coordinator can be restored to drive the two-phase state machine again until the transaction is committed or aborted.

### Isolation

Global Timestamp Service (GTS) generates constantly increasing timestamps in a tenant. It also guarantees the availability of database services based on multiple replicas by using the same replica synchronization mechanism of the replication layer, as described in the preceding section.

When a transaction is committed, GTS generates a timestamp. This timestamp serves as the transaction commit version number and is persisted in the write-ahead log of the log stream. This timestamp is used as the label of all modified data in the transaction.

At the start of each statement (for the Read Committed isolation level) or each transaction (for the Repeatable Read and Serializable isolation levels), a timestamp is obtained from the GTS to serve as the read version number for the statement or transaction. During data reads, versions with timestamps greater than the read version are skipped, and the largest version with a timestamp less than or equal to the read version is selected. This approach ensures a consistent global data snapshot for read operations.

## SQL layer

The SQL layer translates SQL queries of a user into operations on the data of one or more tablets.

### Components at the SQL layer

A query is executed at the SQL layer in the following process: parser, resolver, transformer, optimizer, code generator, and executor.

* The parser performs lexical and syntactic parsing. It divides a user-initiated SQL query into tokens, parses the query based on the predefined syntax rules, and converts the query into a syntax tree.

* The resolver performs semantic parsing. It translates tokens of the SQL query into the corresponding objects such as libraries, tables, columns, and indexes based on the database metadata to generate a statement tree.

* The transformer rewrites the SQL statements in equivalent but different formats based on internal rules or cost models, and then sends the equivalent statements to the optimizer.

  In this process, the transformer performs an equivalent transformation on the original statement tree, and the result of the transformation is still a statement tree.

* The optimizer generates the best execution plan for the SQL query. It selects the access path, join order, and join algorithms, and determines whether to generate a distributed plan by taking into account various factors such as the semantics of the SQL query, characteristics of the objects, and physical distribution of the objects.

* The code generator converts the execution plan into executable code but does not optimize the plan.

* The executor initiates the SQL execution.

In addition to the standard SQL execution process, the SQL layer also provides the plan cache feature. A historical execution plan can be cached in memory and reused by subsequent executions. This avoids repeating the optimization process. Working with the fast-parser module, the SQL layer performs only lexical parsing to parameterize statement strings and obtains the parameterized statement and constant parameters. This way, SQL queries can directly hit the plan cache and the execution of frequently executed SQL queries is accelerated.

### Multiple plan types

Execution plans at the SQL layer are divided into three types, namely local plans, remote plans, and distributed plans. A local execution plan involves only data access at the local server. A remote execution plan involves only data access on a server other than the local server. A distributed execution plan involves data access on more than one server and is divided into multiple sub-plans that are executed on multiple servers.

The SQL layer can divide an execution plan into multiple parts, which are executed in parallel by multiple worker threads based on scheduling rules. Parallel execution makes full use of the CPU and I/O resources and can reduce the response time of a single query. The parallel query technology applies to both distributed and local execution plans.

## Access layer

OceanBase Database Proxy (ODP) serves as the access layer of OceanBase Database. It forwards user requests to the appropriate OceanBase Database instance for processing.

ODP works as an independent process instance and operates separately from the deployment of OceanBase Database instances. It listens on network ports, ensuring compatibility with the MySQL network protocol. This allows applications that use MySQL drivers to directly connect to OceanBase Database.

ODP can automatically discover the data distribution information of the OceanBase cluster. For each SQL statement, ODP strives to recognize the data the statement will access, and directly forwards the statement to the corresponding OceanBase Database instance running on the server where the data resides.

ODP can be deployed on each application server that needs to access OceanBase Database, or on an OBServer node in each zone. In the first deployment mode, an application directly connects to the ODP deployed on the same server, and all queries are routed by the ODP to an appropriate OBServer node. In the second deployment mode, a load balancing service is required to aggregate multiple ODPs and provide a unified service IP address to applications.